{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A+V+T.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNY49fONwEx/cTIUtYHkOVX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manashpratim/Bosch-Summer-Internship/blob/master/A%2BV%2BT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqNRUIuN66rf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "d355be62-ad70-4c9f-94d8-859f64761751"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvjUg_XG7PQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q '/content/drive/My Drive/mosi_data/mosi.zip'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv7ywkSa7cH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to get the file names. Inputs are path and name of the file to be saved\n",
        "def get_file_names(mypath,savefile):\n",
        "  from os import listdir\n",
        "  from os.path import isfile, join\n",
        "  onlyfiles = [f[:f.find('.')] for f in listdir(mypath) if isfile(join(mypath, f))]\n",
        "  with open(savefile, 'w') as f:\n",
        "    for item in onlyfiles:\n",
        "        f.write(item)\n",
        "        f.write('\\n')\n",
        "  return onlyfiles"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-5N4HFp7gsW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mypath = '/content/Raw/Transcript/Segmented'\n",
        "files = get_file_names(mypath,'textfile.txt')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HFxnx0o7jf1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the data\n",
        "import pickle\n",
        "with open('/content/drive/My Drive/mosi_data/labels_joined.pickle', 'rb') as handle:\n",
        "    label= pickle.load(handle)\n",
        "\n",
        "with open('/content/drive/My Drive/mosi_data/text_data_joined.pickle', 'rb') as handle:\n",
        "    dic = pickle.load(handle)\n",
        "\n",
        "with open('/content/drive/My Drive/mosi_data/audio_features_joined.pickle', 'rb') as handle:\n",
        "    dic2 = pickle.load(handle)\n",
        "\n",
        "with open('/content/drive/My Drive/mosi_data/video_features.pickle', 'rb') as handle:\n",
        "    dic3 = pickle.load(handle)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYforFti7y5_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Join all the segments of the text data into a numpy array\n",
        "import numpy as np\n",
        "review = []\n",
        "for key in files:\n",
        "  review+=dic[key]\n",
        "review = np.array(review)"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5XjDw-873qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Join all the labels into a numpy array\n",
        "import numpy as np\n",
        "y = []\n",
        "for key in files:\n",
        "    y+=label[key]\n",
        "y = np.array(y)\n",
        "\n",
        "#y[y>0]=1        #Convert labels to binary\n",
        "#y[y<0]=0\n",
        "\n",
        "y=y.astype(int)   # Execute this line for classification. Comment it for regression\n",
        "ref = {-3:0,-2:1,-1:2,0:3,1:4,2:5,3:6}         #Uncomment the following three lines for 7 class classification\n",
        "for i,num in enumerate(y):\n",
        "  y[i] = ref[num]"
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJr9zgTm771b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This block pads the audio features so that each segments have same length. I use zero padding\n",
        "audio_data = []\n",
        "maximum = float('-inf')\n",
        "max_pad_len = 1639                            #max length of a sequence. For audio_features_joined (MFCC), use this\n",
        "#max_pad_len = 858                            #For audio_pretrained_features_joined (VGGish), use this. Uncomment the above\n",
        "\n",
        "for key in files:\n",
        "  for l in dic2[key]:\n",
        "\n",
        "    if len(l)>0:\n",
        "      #maximum = max(maximum,l.shape[0])\n",
        "      pad_width = max_pad_len - l.shape[0]\n",
        "      mfcc = np.pad(l.T, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "      audio_data.append(mfcc)\n",
        "    else:                                     # This else statement is for the VGGish features data. They have 28 bad frames. It does not affect the MFCC data\n",
        "      f = np.random.rand(128,858)               \n",
        "      audio_data.append(f)\n",
        "\n",
        "audio_data = np.array(audio_data)\n",
        "audio_data= audio_data.reshape(audio_data.shape[0], 57, 1639)             # For audio_features_joined\n",
        "#audio_data= audio_data.reshape(audio_data.shape[0], 128, 858)            # For audio_pretrained_features_joined\n",
        "#audio_data = audio_data.swapaxes(1,-1)"
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4iQu3Oj732J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This block pads the video features so that each segments have same length. I use zero padding\n",
        "video_data = []\n",
        "maximum = float('-inf')\n",
        "max_pad_len = 77                            #max length of a sequence\n",
        "                          \n",
        "\n",
        "for key in files:\n",
        "  for l in dic3[key]:\n",
        "    l = np.array(l)\n",
        "    if len(l)>0:\n",
        "      #maximum = max(maximum,l.shape[0])\n",
        "      pad_width = max_pad_len - l.shape[0]\n",
        "      mfcc = np.pad(l.T, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "      video_data.append(mfcc)\n",
        "    else:                                     # This else statement is for the VGGish features data. They have 28 bad frames. It does not affect the MFCC data\n",
        "      f = np.random.rand(512,77)               \n",
        "      video_data.append(f)\n",
        "\n",
        "video_data = np.array(video_data)\n",
        "video_data= video_data.reshape(video_data.shape[0], 512, 77,1) \n",
        "#video_data = video_data.swapaxes(1,-1)            "
      ],
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNNcWh7t74E7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to generate train-test split. Arguments are text data,labels,audio features data and split_size (0.8 mean 80:20 train-test split)\n",
        "def split_data(text,audio,video,labels,split_size=0.8):\n",
        "  import numpy as np  \n",
        "  train_length =int(len(labels)*split_size)\n",
        "  test_length =int(len(labels)-train_length)\n",
        "  idx = np.random.permutation(labels.shape[0])\n",
        "  text = text[idx]\n",
        "  audio = audio[idx]\n",
        "  video = video[idx]\n",
        "  labels = labels[idx]\n",
        "  text_train = text[:train_length]\n",
        "  text_val = text[train_length:]\n",
        "  labels_train = labels[:train_length]\n",
        "  labels_val = labels[train_length:]\n",
        "  audio_train = audio[:train_length]\n",
        "  audio_val = audio[train_length:]\n",
        "  video_train = video[:train_length]\n",
        "  video_val = video[train_length:]\n",
        "  \n",
        "  return text_train,text_val,audio_train,audio_val,video_train,video_val,labels_train,labels_val"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXey57mrBBdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get train-test split\n",
        "train_reviews, val_reviews, train_audio, val_audio, train_video,val_video,train_labels, val_labels = split_data(review,audio_data,video_data,y,0.8)"
      ],
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRAeHPdbBEFR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "51985e08-deb8-448f-e949-ac0a0a945753"
      },
      "source": [
        "# Preprocess the text data. Similar to the audio data, segments of the text data are paddded to have same length\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_length = 581\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "vocab_len=5000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_len+1,oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(review)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size=len(word_index)\n",
        "print('Size of Vocabulary: ',vocab_size)\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_reviews)\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "val_sequences = tokenizer.texts_to_sequences(val_reviews)\n",
        "val_padded = pad_sequences(val_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "train_labels=np.expand_dims(train_labels, axis=1)\n",
        "val_labels=np.expand_dims(val_labels, axis=1)"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of Vocabulary:  3108\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmT_7XwzBGwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unzip the downloaded embeddings\n",
        "!unzip -q '/content/drive/My Drive/mosi_data/globe6B.zip'"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vJ7DQqSBIe6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the embeddings. There are 4 dimensions to choose from. I used 300 dimensional embeddings. \n",
        "embeddings_index = {}\n",
        "with open('/content/glove.6B.300d.txt') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n96f9-88BKZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Map the embeddings with the words of the text data\n",
        "embedding_dim = 300\n",
        "embeddings_matrix = np.zeros((vocab_size+1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embeddings_matrix[i] = embedding_vector"
      ],
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9tqCDgdDDo9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to compute F1 score. I use it as a metrics for Binary Classification.\n",
        "from keras.callbacks import Callback,ModelCheckpoint\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import keras.backend as K\n",
        "def f1_score(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8PiHSlKA6dc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# text+audio Conatenation fusion model\n",
        "# For using text only model, uncomment the out1 layer. Provide the suitable units inside the Dense layer.\n",
        "# For Binary classification, use 1 and 'sigmoid' as activation\n",
        "# For 7 class classification, use 7 and 'softmax' as activation\n",
        "# For Regression, use 1 and remove activation\n",
        "\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# text only model\n",
        "inp1 = Input(max_length)   \n",
        "layer = tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights = [embeddings_matrix], trainable = False)(inp1)            \n",
        "layer = tf.keras.layers.Dropout(0.4)(layer)\n",
        "layer = tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(128,return_sequences=True))(layer)\n",
        "layer = tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(128))(layer)\n",
        "layer = tf.keras.layers.Dropout(0.2)(layer)\n",
        "layer = tf.keras.layers.Dense(128, activation='relu')(layer)\n",
        "layer = tf.keras.layers.Dropout(0.4)(layer)\n",
        "layer = tf.keras.layers.Dense(64, activation='relu')(layer)\n",
        "layer = tf.keras.layers.Dropout(0.5)(layer)\n",
        "#out1 = tf.keras.layers.Dense(1, activation='sigmoid')(layer)                 \n",
        "model1= Model(inp1,layer)                                 \n",
        "\n",
        "# audio model\n",
        "inp2 = Input((57,1639))           # Dimensions for MFCC data. For VGGish, change it to (128,858)\n",
        "layer2 = tf.keras.layers.Conv1D(64,3,activation='relu',input_shape=(57,1639))(inp2)\n",
        "layer2 = tf.keras.layers.Conv1D(128,3,activation='relu')(layer2)\n",
        "layer2 = tf.keras.layers.MaxPool1D(2)(layer2)\n",
        "layer2 = tf.keras.layers.Conv1D(128,3,activation='relu',padding='same')(layer2)\n",
        "layer2 = tf.keras.layers.Conv1D(256,3,activation='relu')(layer2)\n",
        "layer2 = tf.keras.layers.MaxPool1D(2)(layer2)\n",
        "layer2 = tf.keras.layers.Dropout(0.3)(layer2)\n",
        "layer2 = tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(128,return_sequences=True))(layer2)\n",
        "layer2 = tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(128))(layer2)\n",
        "layer2 = tf.keras.layers.Dropout(0.2)(layer2)\n",
        "layer2 = tf.keras.layers.Dense(256,activation='relu')(layer2)\n",
        "layer2 = tf.keras.layers.Dropout(0.2)(layer2)\n",
        "layer2 = tf.keras.layers.Dense(64,activation='relu')(layer2)\n",
        "#layer2 = tf.keras.layers.Dropout(0.5)(layer2)\n",
        "model2 = Model(inp2,layer2)\n",
        "\n",
        "# audio model\n",
        "inp3 = Input((512,77,1))           \n",
        "layer3 = tf.keras.layers.Conv2D(16,3,activation='relu',use_bias=False,input_shape=(512,77,1))(inp3)\n",
        "layer3 = tf.keras.layers.MaxPool2D(2)(layer3)\n",
        "layer3 = tf.keras.layers.Conv2D(32,3,use_bias=False,activation='relu')(layer3)\n",
        "layer3 = tf.keras.layers.MaxPool2D(2)(layer3)\n",
        "layer3 = tf.keras.layers.Conv2D(64,3,use_bias=False,activation='relu')(layer3)\n",
        "layer3 = tf.keras.layers.MaxPool2D(2)(layer3)\n",
        "layer3 = tf.keras.layers.Conv2D(128,3,use_bias=False,activation='relu')(layer3)\n",
        "layer3 = tf.keras.layers.MaxPool2D(2)(layer3)\n",
        "layer3 = tf.keras.layers.Dropout(0.3)(layer3)\n",
        "#layer3 = tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(128,return_sequences=True))(layer3)\n",
        "#layer3 = tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(128))(layer3)\n",
        "#layer3 = tf.keras.layers.Dropout(0.2)(layer3)\n",
        "layer3 = tf.keras.layers.Flatten()(layer3)\n",
        "layer3 = tf.keras.layers.Dense(256,activation='relu')(layer3)\n",
        "layer3 = tf.keras.layers.Dropout(0.2)(layer3)\n",
        "layer3 = tf.keras.layers.Dense(64,activation='relu')(layer3)\n",
        "#layer2 = tf.keras.layers.Dropout(0.5)(layer2)\n",
        "model3 = Model(inp3,layer3)\n",
        "\n",
        "# Fusion of the two models. I concatenate the two models and pass it through a projection layer. Bothe the text and audio models output 64 dimensional vectors\n",
        "# So the dense layer has 128 units\n",
        "fusion = tf.keras.layers.Concatenate(axis=1)([model1.output, model2.output,model3.output])\n",
        "#fusion = tf.keras.layers.BatchNormalization()(fusion)    \n",
        "fusion = tf.keras.layers.Dense(192,activation='relu')(fusion)\n",
        "#fusion = tf.keras.layers.Dropout(0.5)(fusion)\n",
        "out = tf.keras.layers.Dense(1, activation='sigmoid')(fusion)        #For binary classification. For 7 class, change it to 7 and 'softmax'   \n",
        "#out = tf.keras.layers.Dense(1)(fusion)      #uncomment for regression. comment the above line\n",
        "#out = tf.keras.layers.Dense(7, activation='softmax')(fusion)\n",
        "model = Model([model1.input,model2.input,model3.input],out)             #the fused model"
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6MEU5A074nP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "02e0e5b2-63bc-4a10-a5ec-b9e2e7dab3ff"
      },
      "source": [
        "reduce =tf. keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=10, mode='auto')  #to reduce learning rate by factor of 0.1 if model performance degrades for 10 (patience) epochs.  \n",
        "#early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=10, mode='auto')  #early stopping if performance of model degrades for 10 epochs\n",
        "\n",
        "#Uncomment one of the next three lines at a time\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy',f1_score])             #Binary classification\n",
        "#model.compile(loss='sparse_categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])            #7 class classifiaction\n",
        "#model.compile(loss=\"mean_absolute_error\",optimizer='adam',metrics=[\"mean_absolute_error\"])             #Regression\n",
        "\n",
        "# I am training for 50 epochs with a batch size of 256. Set verbose to 2 for no training details and 0 for more training details.\n",
        "num_epochs = 50\n",
        "history=model.fit([train_padded,train_audio,train_video], \n",
        "                    train_labels, \n",
        "                    epochs=num_epochs, \n",
        "                    batch_size=64, \n",
        "                    validation_data=([val_padded,val_audio,val_video],val_labels),\n",
        "                    callbacks=[reduce],\n",
        "                    verbose=1)"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "28/28 [==============================] - 8s 271ms/step - loss: 5.8591 - accuracy: 0.2365 - val_loss: 1.7139 - val_accuracy: 0.2773 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "28/28 [==============================] - 7s 242ms/step - loss: 1.8079 - accuracy: 0.2513 - val_loss: 1.6375 - val_accuracy: 0.2977 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "28/28 [==============================] - 7s 241ms/step - loss: 1.7130 - accuracy: 0.2655 - val_loss: 1.6540 - val_accuracy: 0.3045 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "28/28 [==============================] - 7s 242ms/step - loss: 1.6662 - accuracy: 0.2968 - val_loss: 1.5894 - val_accuracy: 0.3091 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "28/28 [==============================] - 7s 243ms/step - loss: 1.6253 - accuracy: 0.3115 - val_loss: 1.5381 - val_accuracy: 0.3364 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "28/28 [==============================] - 7s 242ms/step - loss: 1.5893 - accuracy: 0.3024 - val_loss: 1.5308 - val_accuracy: 0.3455 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "28/28 [==============================] - 7s 243ms/step - loss: 1.5611 - accuracy: 0.3320 - val_loss: 1.4807 - val_accuracy: 0.3523 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "28/28 [==============================] - 7s 241ms/step - loss: 1.5042 - accuracy: 0.3650 - val_loss: 1.5811 - val_accuracy: 0.3432 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "28/28 [==============================] - 7s 241ms/step - loss: 1.4843 - accuracy: 0.3570 - val_loss: 1.5643 - val_accuracy: 0.3136 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "28/28 [==============================] - 7s 243ms/step - loss: 1.4592 - accuracy: 0.3792 - val_loss: 1.5712 - val_accuracy: 0.2864 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "28/28 [==============================] - 7s 241ms/step - loss: 1.4289 - accuracy: 0.3917 - val_loss: 1.4431 - val_accuracy: 0.3682 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "28/28 [==============================] - 7s 242ms/step - loss: 1.4091 - accuracy: 0.3746 - val_loss: 1.5236 - val_accuracy: 0.3250 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "28/28 [==============================] - 7s 242ms/step - loss: 1.3912 - accuracy: 0.3883 - val_loss: 1.4340 - val_accuracy: 0.3614 - lr: 0.0010\n",
            "Epoch 14/50\n",
            "28/28 [==============================] - 7s 243ms/step - loss: 1.3519 - accuracy: 0.4053 - val_loss: 1.4752 - val_accuracy: 0.3591 - lr: 0.0010\n",
            "Epoch 15/50\n",
            "28/28 [==============================] - 7s 240ms/step - loss: 1.3250 - accuracy: 0.4457 - val_loss: 1.5395 - val_accuracy: 0.3886 - lr: 0.0010\n",
            "Epoch 16/50\n",
            "28/28 [==============================] - 7s 244ms/step - loss: 1.2824 - accuracy: 0.4440 - val_loss: 1.5422 - val_accuracy: 0.3477 - lr: 0.0010\n",
            "Epoch 17/50\n",
            "28/28 [==============================] - 7s 243ms/step - loss: 1.2740 - accuracy: 0.4434 - val_loss: 1.4670 - val_accuracy: 0.3614 - lr: 0.0010\n",
            "Epoch 18/50\n",
            "28/28 [==============================] - 7s 242ms/step - loss: 1.2217 - accuracy: 0.4417 - val_loss: 1.4815 - val_accuracy: 0.3932 - lr: 0.0010\n",
            "Epoch 19/50\n",
            "28/28 [==============================] - 7s 241ms/step - loss: 1.2243 - accuracy: 0.4821 - val_loss: 1.5511 - val_accuracy: 0.4136 - lr: 0.0010\n",
            "Epoch 20/50\n",
            "28/28 [==============================] - 7s 243ms/step - loss: 1.1779 - accuracy: 0.4929 - val_loss: 1.5173 - val_accuracy: 0.3750 - lr: 0.0010\n",
            "Epoch 21/50\n",
            "28/28 [==============================] - 7s 245ms/step - loss: 1.3465 - accuracy: 0.4622 - val_loss: 1.6392 - val_accuracy: 0.2477 - lr: 0.0010\n",
            "Epoch 22/50\n",
            "28/28 [==============================] - 7s 248ms/step - loss: 1.4664 - accuracy: 0.3729 - val_loss: 1.4381 - val_accuracy: 0.3909 - lr: 0.0010\n",
            "Epoch 23/50\n",
            "28/28 [==============================] - 7s 249ms/step - loss: 1.3484 - accuracy: 0.4275 - val_loss: 1.5533 - val_accuracy: 0.3000 - lr: 0.0010\n",
            "Epoch 24/50\n",
            "28/28 [==============================] - 7s 250ms/step - loss: 1.3697 - accuracy: 0.4122 - val_loss: 1.4549 - val_accuracy: 0.3773 - lr: 0.0010\n",
            "Epoch 25/50\n",
            "28/28 [==============================] - 7s 251ms/step - loss: 1.2849 - accuracy: 0.4537 - val_loss: 1.4783 - val_accuracy: 0.3409 - lr: 0.0010\n",
            "Epoch 26/50\n",
            "28/28 [==============================] - 7s 251ms/step - loss: 1.2404 - accuracy: 0.4719 - val_loss: 1.7720 - val_accuracy: 0.3023 - lr: 0.0010\n",
            "Epoch 27/50\n",
            "28/28 [==============================] - 7s 249ms/step - loss: 1.2040 - accuracy: 0.5105 - val_loss: 1.5257 - val_accuracy: 0.3273 - lr: 0.0010\n",
            "Epoch 28/50\n",
            "28/28 [==============================] - 7s 250ms/step - loss: 1.1837 - accuracy: 0.5094 - val_loss: 1.5750 - val_accuracy: 0.3545 - lr: 0.0010\n",
            "Epoch 29/50\n",
            "28/28 [==============================] - 7s 251ms/step - loss: 1.1164 - accuracy: 0.5281 - val_loss: 1.5720 - val_accuracy: 0.3545 - lr: 0.0010\n",
            "Epoch 30/50\n",
            " 5/28 [====>.........................] - ETA: 4s - loss: 1.0989 - accuracy: 0.5063"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-215-604a45fb793f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_padded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_audio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_video\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                     verbose=1)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    853\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \"\"\"\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_process_logs\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;34m\"\"\"Turns tensors into numpy arrays or Python scalars.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    517\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \"\"\"\n\u001b[1;32m    960\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyHdMPbdQn8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "#from keras import regularizers\n",
        "\n",
        "model2=tf.keras.models.Sequential([\n",
        "    \n",
        "    tf.keras.layers.Conv2D(16,(3,3),activation='relu',input_shape=(512,77,1)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(32,(3,3),activation='relu'),\n",
        "    #tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(64,(3,3),activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128,(3,3),activation='relu',padding='same'),\n",
        "    #tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    #tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(128,return_sequences=True)),\n",
        "    #tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(128)),\n",
        "    tf.keras.layers.Dense(256,activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(64,activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    #tf.keras.layers.Dense(1)\n",
        "    #tf.keras.layers.Dense(1,activation='sigmoid')\n",
        "    tf.keras.layers.Dense(7,activation='softmax')\n",
        "])"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoCABane74Ch",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "31ac37a0-570a-4e5f-c2b3-85c0c6337d34"
      },
      "source": [
        "reduce =tf. keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, mode='auto')  #to reduce learning rate by factor of 0.1 if model performance degrades for 10 (patience) epochs.  \n",
        "#early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=10, mode='auto')  #early stopping if performance of model degrades for 10 epochs\n",
        "\n",
        "#Uncomment one of the next three lines at a time\n",
        "#model2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy',f1_score])                #Binary classification\n",
        "model2.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])            #7 class classifiaction\n",
        "#model2.compile(loss=\"mean_absolute_error\",optimizer='adam',metrics=[\"mean_absolute_error\"])             #Regression\n",
        "\n",
        "# I am training for 50 epochs with a batch size of 256. Set verbose to 2 for no training details and 0 for more training details.\n",
        "num_epochs = 50\n",
        "history=model2.fit(train_video, \n",
        "                    train_labels, \n",
        "                    epochs=num_epochs, \n",
        "                    batch_size=256, \n",
        "                    validation_data=(val_video,val_labels),\n",
        "                    callbacks=[reduce],\n",
        "                    verbose=1)"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "7/7 [==============================] - 2s 219ms/step - loss: 2.4446 - accuracy: 0.2155 - val_loss: 1.8566 - val_accuracy: 0.2932 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "7/7 [==============================] - 1s 197ms/step - loss: 1.8279 - accuracy: 0.2530 - val_loss: 1.7280 - val_accuracy: 0.3045 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "7/7 [==============================] - 1s 199ms/step - loss: 1.7412 - accuracy: 0.2769 - val_loss: 1.6951 - val_accuracy: 0.3023 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "7/7 [==============================] - 1s 198ms/step - loss: 1.7171 - accuracy: 0.2763 - val_loss: 1.6454 - val_accuracy: 0.3045 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "7/7 [==============================] - 1s 200ms/step - loss: 1.7028 - accuracy: 0.2854 - val_loss: 1.6713 - val_accuracy: 0.2977 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "7/7 [==============================] - 1s 199ms/step - loss: 1.6821 - accuracy: 0.2803 - val_loss: 1.6384 - val_accuracy: 0.3000 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "7/7 [==============================] - 1s 197ms/step - loss: 1.6718 - accuracy: 0.2899 - val_loss: 1.6429 - val_accuracy: 0.2955 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "7/7 [==============================] - 1s 198ms/step - loss: 1.6611 - accuracy: 0.2837 - val_loss: 1.6306 - val_accuracy: 0.3000 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "7/7 [==============================] - 1s 198ms/step - loss: 1.6665 - accuracy: 0.2888 - val_loss: 1.6394 - val_accuracy: 0.3068 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "7/7 [==============================] - 1s 198ms/step - loss: 1.6519 - accuracy: 0.3036 - val_loss: 1.6291 - val_accuracy: 0.3023 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "7/7 [==============================] - 1s 198ms/step - loss: 1.6468 - accuracy: 0.2911 - val_loss: 1.6340 - val_accuracy: 0.3068 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "7/7 [==============================] - 1s 199ms/step - loss: 1.6466 - accuracy: 0.2877 - val_loss: 1.6286 - val_accuracy: 0.3068 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "7/7 [==============================] - 1s 198ms/step - loss: 1.6454 - accuracy: 0.2996 - val_loss: 1.6289 - val_accuracy: 0.3068 - lr: 0.0010\n",
            "Epoch 14/50\n",
            "7/7 [==============================] - 1s 198ms/step - loss: 1.6367 - accuracy: 0.2922 - val_loss: 1.6309 - val_accuracy: 0.3068 - lr: 0.0010\n",
            "Epoch 15/50\n",
            "7/7 [==============================] - 1s 199ms/step - loss: 1.6337 - accuracy: 0.2951 - val_loss: 1.6305 - val_accuracy: 0.3068 - lr: 0.0010\n",
            "Epoch 16/50\n",
            "7/7 [==============================] - 1s 199ms/step - loss: 1.6143 - accuracy: 0.2973 - val_loss: 1.6355 - val_accuracy: 0.3023 - lr: 0.0010\n",
            "Epoch 17/50\n",
            "7/7 [==============================] - 1s 199ms/step - loss: 1.6171 - accuracy: 0.3013 - val_loss: 1.6376 - val_accuracy: 0.3000 - lr: 0.0010\n",
            "Epoch 18/50\n",
            "7/7 [==============================] - 1s 198ms/step - loss: 1.6264 - accuracy: 0.2797 - val_loss: 1.6425 - val_accuracy: 0.3000 - lr: 0.0010\n",
            "Epoch 19/50\n",
            "7/7 [==============================] - 1s 199ms/step - loss: 1.6086 - accuracy: 0.3098 - val_loss: 1.6406 - val_accuracy: 0.2932 - lr: 0.0010\n",
            "Epoch 20/50\n",
            "7/7 [==============================] - 1s 197ms/step - loss: 1.6132 - accuracy: 0.2899 - val_loss: 1.6398 - val_accuracy: 0.2977 - lr: 0.0010\n",
            "Epoch 21/50\n",
            "7/7 [==============================] - 1s 199ms/step - loss: 1.6131 - accuracy: 0.2985 - val_loss: 1.6355 - val_accuracy: 0.3068 - lr: 0.0010\n",
            "Epoch 22/50\n",
            "7/7 [==============================] - 1s 198ms/step - loss: 1.6020 - accuracy: 0.3098 - val_loss: 1.6382 - val_accuracy: 0.3159 - lr: 0.0010\n",
            "Epoch 23/50\n",
            "7/7 [==============================] - 1s 198ms/step - loss: 1.6050 - accuracy: 0.3081 - val_loss: 1.6377 - val_accuracy: 0.3068 - lr: 1.0000e-04\n",
            "Epoch 24/50\n",
            "7/7 [==============================] - 1s 201ms/step - loss: 1.6055 - accuracy: 0.3070 - val_loss: 1.6377 - val_accuracy: 0.3068 - lr: 1.0000e-04\n",
            "Epoch 25/50\n",
            "7/7 [==============================] - 1s 198ms/step - loss: 1.5995 - accuracy: 0.3013 - val_loss: 1.6380 - val_accuracy: 0.3068 - lr: 1.0000e-04\n",
            "Epoch 26/50\n",
            "7/7 [==============================] - 1s 199ms/step - loss: 1.5890 - accuracy: 0.3115 - val_loss: 1.6388 - val_accuracy: 0.3068 - lr: 1.0000e-04\n",
            "Epoch 27/50\n",
            "7/7 [==============================] - 1s 198ms/step - loss: 1.5965 - accuracy: 0.3024 - val_loss: 1.6394 - val_accuracy: 0.3068 - lr: 1.0000e-04\n",
            "Epoch 28/50\n",
            "7/7 [==============================] - 1s 198ms/step - loss: 1.5902 - accuracy: 0.3161 - val_loss: 1.6401 - val_accuracy: 0.3068 - lr: 1.0000e-04\n",
            "Epoch 29/50\n",
            "7/7 [==============================] - 1s 200ms/step - loss: 1.6002 - accuracy: 0.3155 - val_loss: 1.6404 - val_accuracy: 0.3068 - lr: 1.0000e-04\n",
            "Epoch 30/50\n",
            "7/7 [==============================] - 1s 199ms/step - loss: 1.6075 - accuracy: 0.2996 - val_loss: 1.6409 - val_accuracy: 0.3159 - lr: 1.0000e-04\n",
            "Epoch 31/50\n",
            "7/7 [==============================] - 1s 200ms/step - loss: 1.6206 - accuracy: 0.3110 - val_loss: 1.6402 - val_accuracy: 0.3159 - lr: 1.0000e-04\n",
            "Epoch 32/50\n",
            "7/7 [==============================] - 1s 201ms/step - loss: 1.6072 - accuracy: 0.3047 - val_loss: 1.6406 - val_accuracy: 0.3159 - lr: 1.0000e-04\n",
            "Epoch 33/50\n",
            "7/7 [==============================] - 1s 199ms/step - loss: 1.5937 - accuracy: 0.3115 - val_loss: 1.6406 - val_accuracy: 0.3159 - lr: 1.0000e-05\n",
            "Epoch 34/50\n",
            "7/7 [==============================] - 1s 202ms/step - loss: 1.5962 - accuracy: 0.3138 - val_loss: 1.6406 - val_accuracy: 0.3159 - lr: 1.0000e-05\n",
            "Epoch 35/50\n",
            "7/7 [==============================] - 1s 199ms/step - loss: 1.5996 - accuracy: 0.3172 - val_loss: 1.6406 - val_accuracy: 0.3159 - lr: 1.0000e-05\n",
            "Epoch 36/50\n",
            "7/7 [==============================] - 1s 198ms/step - loss: 1.5963 - accuracy: 0.3098 - val_loss: 1.6407 - val_accuracy: 0.3159 - lr: 1.0000e-05\n",
            "Epoch 37/50\n",
            "7/7 [==============================] - 1s 198ms/step - loss: 1.6011 - accuracy: 0.3076 - val_loss: 1.6408 - val_accuracy: 0.3159 - lr: 1.0000e-05\n",
            "Epoch 38/50\n",
            "7/7 [==============================] - 1s 197ms/step - loss: 1.5972 - accuracy: 0.3019 - val_loss: 1.6410 - val_accuracy: 0.3159 - lr: 1.0000e-05\n",
            "Epoch 39/50\n",
            "7/7 [==============================] - 1s 198ms/step - loss: 1.5948 - accuracy: 0.3223 - val_loss: 1.6410 - val_accuracy: 0.3159 - lr: 1.0000e-05\n",
            "Epoch 40/50\n",
            "7/7 [==============================] - 1s 198ms/step - loss: 1.5973 - accuracy: 0.3167 - val_loss: 1.6411 - val_accuracy: 0.3159 - lr: 1.0000e-05\n",
            "Epoch 41/50\n",
            "7/7 [==============================] - 1s 198ms/step - loss: 1.6003 - accuracy: 0.3229 - val_loss: 1.6411 - val_accuracy: 0.3159 - lr: 1.0000e-05\n",
            "Epoch 42/50\n",
            "7/7 [==============================] - 1s 198ms/step - loss: 1.6001 - accuracy: 0.3161 - val_loss: 1.6411 - val_accuracy: 0.3159 - lr: 1.0000e-05\n",
            "Epoch 43/50\n",
            "7/7 [==============================] - 1s 201ms/step - loss: 1.5974 - accuracy: 0.3007 - val_loss: 1.6411 - val_accuracy: 0.3159 - lr: 1.0000e-06\n",
            "Epoch 44/50\n",
            "7/7 [==============================] - 1s 198ms/step - loss: 1.6091 - accuracy: 0.3070 - val_loss: 1.6411 - val_accuracy: 0.3159 - lr: 1.0000e-06\n",
            "Epoch 45/50\n",
            "7/7 [==============================] - 1s 198ms/step - loss: 1.6053 - accuracy: 0.3064 - val_loss: 1.6411 - val_accuracy: 0.3159 - lr: 1.0000e-06\n",
            "Epoch 46/50\n",
            "7/7 [==============================] - 1s 198ms/step - loss: 1.5902 - accuracy: 0.3132 - val_loss: 1.6411 - val_accuracy: 0.3159 - lr: 1.0000e-06\n",
            "Epoch 47/50\n",
            "7/7 [==============================] - 1s 198ms/step - loss: 1.5914 - accuracy: 0.2962 - val_loss: 1.6411 - val_accuracy: 0.3159 - lr: 1.0000e-06\n",
            "Epoch 48/50\n",
            "7/7 [==============================] - 1s 199ms/step - loss: 1.5948 - accuracy: 0.3121 - val_loss: 1.6412 - val_accuracy: 0.3159 - lr: 1.0000e-06\n",
            "Epoch 49/50\n",
            "7/7 [==============================] - 1s 198ms/step - loss: 1.6079 - accuracy: 0.3110 - val_loss: 1.6412 - val_accuracy: 0.3159 - lr: 1.0000e-06\n",
            "Epoch 50/50\n",
            "7/7 [==============================] - 1s 196ms/step - loss: 1.5845 - accuracy: 0.3138 - val_loss: 1.6412 - val_accuracy: 0.3159 - lr: 1.0000e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jymQ3kI74AR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roj2Umky7381",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbVZGbj173zm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICfy7g4d73w4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Customized layer for weighted sum\n",
        "class WeightedSum(tf.keras.layers.Layer):\n",
        "    def __init__(self, a, **kwargs):\n",
        "        self.a = a\n",
        "        super(WeightedSum, self).__init__(**kwargs)\n",
        "    def call(self, model_outputs):\n",
        "        return self.a * model_outputs[0] + (1 - self.a) * model_outputs[1]\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi9WS4tg73vN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# text+audio weighted sum of logits fusion model\n",
        "\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "#text model\n",
        "inp1 = Input(max_length)   \n",
        "layer = tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights = [embeddings_matrix], trainable = False)(inp1)            \n",
        "layer = tf.keras.layers.Dropout(0.4)(layer)\n",
        "layer = tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(128,return_sequences=True))(layer)\n",
        "layer = tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(128))(layer)\n",
        "layer = tf.keras.layers.Dropout(0.2)(layer)\n",
        "layer = tf.keras.layers.Dense(128, activation='relu')(layer)\n",
        "layer = tf.keras.layers.Dropout(0.4)(layer)\n",
        "layer = tf.keras.layers.Dense(64, activation='relu')(layer)\n",
        "layer = tf.keras.layers.Dropout(0.5)(layer)\n",
        "out1 = tf.keras.layers.Dense(1)(layer)                  #Use 7 for 7 class classification. Do not change for binary classification and regression\n",
        "model1= Model(inp1,out1)\n",
        "\n",
        "#audio model\n",
        "inp2 = Input((128,858))\n",
        "layer2 = tf.keras.layers.Conv1D(64,3,activation='relu',input_shape=(128,858))(inp2)\n",
        "layer2 = tf.keras.layers.Conv1D(128,3,activation='relu')(layer2)\n",
        "layer2 = tf.keras.layers.MaxPool1D(2)(layer2)\n",
        "layer2 = tf.keras.layers.Conv1D(128,3,activation='relu',padding='same')(layer2)\n",
        "layer2 = tf.keras.layers.Conv1D(256,3,activation='relu')(layer2)\n",
        "layer2 = tf.keras.layers.Dropout(0.3)(layer2)\n",
        "layer2 = tf.keras.layers.MaxPool1D(2)(layer2)\n",
        "layer2 = tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(128,return_sequences=True))(layer2)\n",
        "layer2 = tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(128))(layer2)\n",
        "layer2 = tf.keras.layers.Dropout(0.2)(layer2)\n",
        "layer2 = tf.keras.layers.Dense(256,activation='relu')(layer2)\n",
        "layer2 = tf.keras.layers.Dropout(0.2)(layer2)\n",
        "layer2 = tf.keras.layers.Dense(64,activation='relu')(layer2)\n",
        "layer2 = tf.keras.layers.Dropout(0.3)(layer2)\n",
        "out2 = tf.keras.layers.Dense(1)(layer2)                         #Use 7 for 7 class classification. Do not change for binary classification and regression\n",
        "model2 = Model(inp2,out2)\n",
        "\n",
        "# 0.5 in the next line mean 50% weightage to both modalities\n",
        "fusion = WeightedSum(0.5)([model1.output, model2.output])\n",
        "\n",
        "#For regression, comment both the lines\n",
        "out = tf.keras.layers.Activation('sigmoid')(fusion)             \n",
        "#out = tf.keras.layers.Activation('softmax')(fusion)            #uncomment for 7 class classification\n",
        "     \n",
        "\n",
        "model3 = Model([model1.input,model2.input],out)             # for regression, change 'out' to 'fusion'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}