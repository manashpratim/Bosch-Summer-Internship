{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AudioPretrained.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPAGRJ1Umjl4Xo7hQ+/uitt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manashpratim/Bosch-Summer-Internship/blob/master/AudioPretrained.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lhe1vrdXdDNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data is available at https://drive.google.com/drive/folders/1NFYIaXjL8V5kvZo3g9JEafLQ3scslWic?usp=sharing\n",
        "\n",
        "#Execute all the lines of the notebook sequentially to generate pretrained audio features "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFFfPEz2cmXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Wkws-93aYIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install numpy scipy\n",
        "!pip install resampy tensorflow six\n",
        "!pip install tf-slim\n",
        "!git clone https://github.com/tensorflow/models.git\n",
        "!curl -O https://storage.googleapis.com/audioset/vggish_model.ckpt\n",
        "!curl -O https://storage.googleapis.com/audioset/vggish_pca_params.npz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAey8DZVbNM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TRfQ88tbOfo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls models/research/audioset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSMrxxFuboXz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r models/research/audioset/* ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXGMXuUwbfvA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/vggish/* /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC8624Rjbsr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5myOqyYjbusJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from vggish_smoke_test import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcBCNxPab0qa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import vggish_slim\n",
        "import vggish_params\n",
        "import vggish_input\n",
        "\n",
        "def CreateVGGishNetwork(hop_size=0.96):   # Hop size is in seconds.\n",
        "  \"\"\"Define VGGish model, load the checkpoint, and return a dictionary that points\n",
        "  to the different tensors defined by the model.\n",
        "  \"\"\"\n",
        "  vggish_slim.define_vggish_slim()\n",
        "  checkpoint_path = 'vggish_model.ckpt'\n",
        "  vggish_params.EXAMPLE_HOP_SECONDS = hop_size\n",
        "  \n",
        "  vggish_slim.load_vggish_slim_checkpoint(sess, checkpoint_path)\n",
        "\n",
        "  features_tensor = sess.graph.get_tensor_by_name(\n",
        "      vggish_params.INPUT_TENSOR_NAME)\n",
        "  embedding_tensor = sess.graph.get_tensor_by_name(\n",
        "      vggish_params.OUTPUT_TENSOR_NAME)\n",
        "\n",
        "  layers = {'conv1': 'vggish/conv1/Relu',\n",
        "            'pool1': 'vggish/pool1/MaxPool',\n",
        "            'conv2': 'vggish/conv2/Relu',\n",
        "            'pool2': 'vggish/pool2/MaxPool',\n",
        "            'conv3': 'vggish/conv3/conv3_2/Relu',\n",
        "            'pool3': 'vggish/pool3/MaxPool',\n",
        "            'conv4': 'vggish/conv4/conv4_2/Relu',\n",
        "            'pool4': 'vggish/pool4/MaxPool',\n",
        "            'fc1': 'vggish/fc1/fc1_2/Relu',\n",
        "            'fc2': 'vggish/fc2/Relu',\n",
        "            'embedding': 'vggish/embedding',\n",
        "            'features': 'vggish/input_features',\n",
        "         }\n",
        "  g = tf.compat.v1.get_default_graph()\n",
        "  for k in layers:\n",
        "    layers[k] = g.get_tensor_by_name( layers[k] + ':0')\n",
        "    \n",
        "  return {'features': features_tensor,\n",
        "          'embedding': embedding_tensor,\n",
        "          'layers': layers,\n",
        "         }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLtaxIa0b1pw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ProcessWithVGGish(vgg, x, sr):\n",
        "  '''Run the VGGish model, starting with a sound (x) at sample rate\n",
        "  (sr). Return a whitened version of the embeddings. Sound must be scaled to be\n",
        "  floats between -1 and +1.'''\n",
        "\n",
        "  # Produce a batch of log mel spectrogram examples.\n",
        "  input_batch = vggish_input.waveform_to_examples(x, sr)\n",
        "  # print('Log Mel Spectrogram example: ', input_batch[0])\n",
        "\n",
        "  [embedding_batch] = sess.run([vgg['embedding']],\n",
        "                               feed_dict={vgg['features']: input_batch})\n",
        "\n",
        "  # Postprocess the results to produce whitened quantized embeddings.\n",
        "  pca_params_path = 'vggish_pca_params.npz'\n",
        "\n",
        "  pproc = vggish_postprocess.Postprocessor(pca_params_path)\n",
        "  postprocessed_batch = pproc.postprocess(embedding_batch)\n",
        "  # print('Postprocessed VGGish embedding: ', postprocessed_batch[0])\n",
        "  return postprocessed_batch[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMsg_1Sdb5-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU-rWQ_Tb8mV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test these new functions with the original test.\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.reset_default_graph()\n",
        "sess = tf.compat.v1.Session()\n",
        "vgg = CreateVGGishNetwork(0.06)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTVk61EucAQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def EmbeddingsFromVGGish(vgg, x, sr):\n",
        "  '''Run the VGGish model, starting with a sound (x) at sample rate\n",
        "  (sr). Return a dictionary of embeddings from the different layers\n",
        "  of the model.'''\n",
        "  # Produce a batch of log mel spectrogram examples.\n",
        "  input_batch = vggish_input.waveform_to_examples(x, sr)\n",
        "  # print('Log Mel Spectrogram example: ', input_batch[0])\n",
        "\n",
        "  layer_names = vgg['layers'].keys()\n",
        "  tensors = [vgg['layers'][k] for k in layer_names]\n",
        "  \n",
        "  results = sess.run(tensors,\n",
        "                     feed_dict={vgg['features']: input_batch})\n",
        "\n",
        "  resdict = {}\n",
        "  for i, k in enumerate(layer_names):\n",
        "    resdict[k] = results[i]\n",
        "    \n",
        "  return resdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95aTZYIdcEdf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Unzip the MOSI data\n",
        "!unzip -q '/content/drive/My Drive/mosi_data/mosi.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At1-FDwIc2Tt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to get the audio file name. Arguments are path of the audio files and the name of the save file\n",
        "def get_file_names(mypath,savefile):\n",
        "  from os import listdir\n",
        "  from os.path import isfile, join\n",
        "  onlyfiles = [f[:f.find('.')] for f in listdir(mypath) if isfile(join(mypath, f))]\n",
        "  with open(savefile, 'w') as f:\n",
        "    for item in onlyfiles:\n",
        "        f.write(item)\n",
        "        f.write('\\n')\n",
        "  return onlyfiles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoWST0i0c4bn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mypath = '/content/Raw/Audio/WAV_16000/Segmented' \n",
        "savefile = 'audiofiles.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7HKk-FOc5ja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "audiofiles = get_file_names(mypath,savefile)              #Get the names of the audio files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5YI2B2Ec-Cj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "#Function to get the pretrained audio features\n",
        "def get_audio_features(mypath,audiofiles):\n",
        "    import librosa\n",
        "    import numpy as np\n",
        "    mydic = {}\n",
        "    outer = tqdm(total=len(audiofiles), desc='Extracting...', position=0)\n",
        "    for file in audiofiles:\n",
        "      outer.update(1)\n",
        "      audiofile = mypath + '/' + file + '.wav'\n",
        "      x, sr = librosa.load(audiofile, sr  = 16000, res_type='kaiser_fast')        #I am extracting frames at 16KHz. \n",
        "      try:\n",
        "        resdict = EmbeddingsFromVGGish(vgg, x, sr)\n",
        "        mydic[file] = resdict['embedding']\n",
        "      #Some of the segments are bad. If you use run the notebook without changing any parameters, there will be 28 bad segments out of 2199\n",
        "      except:                                                           \n",
        "        mydic[file] = []\n",
        "        pass\n",
        "   \n",
        "    return mydic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQEkaouldRj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "audio =  get_audio_features(mypath,audiofiles)              #get the pretrained audio features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8KAGk0G5E-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This block changes the format of the audio data to match the other data modalities\n",
        "files = audiofiles\n",
        "dic = {}\n",
        "for file in files:\n",
        "  new = file[:-file[::-1].find('_')-1]\n",
        "  dic[file] = new\n",
        "newdic = {}\n",
        "\n",
        "newdic1 = {}\n",
        "for key in audio:\n",
        "  newkey = dic[key]\n",
        "  if newkey not in newdic:\n",
        "    newdic[newkey] = {}\n",
        "  if newkey not in newdic1:\n",
        "    newdic1[newkey] = {}\n",
        "  newdic1[newkey][key] = audio[key]\n",
        "  k = key[-key[::-1].find('_'):]\n",
        "  newdic[newkey][int(k)] = audio[key]\n",
        "\n",
        "nd = {}\n",
        "for key in newdic:\n",
        "    nd[key] = []\n",
        "    for k in sorted(newdic[key].keys()):\n",
        "      nd[key].append(newdic[key][k])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZST0dR_dw7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save the pretrained audio features as a pickle file\n",
        "import pickle\n",
        "with open('/content/drive/My Drive/mosi_data/audio_pretrained_features_joined.pickle', 'wb') as handle:\n",
        "    pickle.dump(nd, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}