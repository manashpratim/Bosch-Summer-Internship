{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOYxkZ1L/D3au05ocbYLX9A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manashpratim/Bosch-Summer-Internship/blob/master/Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGh6EFWic8K2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data is available at https://drive.google.com/drive/folders/1NFYIaXjL8V5kvZo3g9JEafLQ3scslWic?usp=sharing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy-64GNwGF6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWaXFaGYEkbB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Unzip the dataset\n",
        "!unzip -q '/content/drive/My Drive/mosi_data/mosi.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTNEHXZ8JWwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to get the file names. Inputs are path and name of the file to be saved\n",
        "def get_file_names(mypath,savefile):\n",
        "  from os import listdir\n",
        "  from os.path import isfile, join\n",
        "  onlyfiles = [f[:f.find('.')] for f in listdir(mypath) if isfile(join(mypath, f))]\n",
        "  with open(savefile, 'w') as f:\n",
        "    for item in onlyfiles:\n",
        "        f.write(item)\n",
        "        f.write('\\n')\n",
        "  return onlyfiles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjzjTwU9JYO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#specify the path and get the file\n",
        "mypath = '/content/Raw/Transcript/Segmented'\n",
        "files = get_file_names(mypath,'textfile.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1dbPy64kgW7",
        "colab_type": "text"
      },
      "source": [
        "# **Extracting Text Data from Transcripts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqZOyGqOkoyA",
        "colab_type": "text"
      },
      "source": [
        "The extracted data is available at the link provided above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF61bU90Ktlt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to process the transcripts and save them as pickle file. \n",
        "# Arguments are location of the transcripts, name of the files and the name of the file to be saved.\n",
        "def get_text_data_joined(mypath,files,savefile):\n",
        "  dic = {}\n",
        "  for file in files:\n",
        "      filename = mypath+'/' + file + '.annotprocessed'\n",
        "      text_file = open(filename, \"r\")\n",
        "      lines = text_file.read().split('\\n')\n",
        "      lines = lines[:-1]\n",
        "      dic[file] = []\n",
        "      for line in lines:\n",
        "        ind1 = line.find('_')+1\n",
        "        line = line[ind1:]\n",
        "        ind2 = line.find('_')+1\n",
        "        line = line[ind2:].strip()\n",
        "        dic[file].append(line.lower())\n",
        "  import pickle\n",
        "  with open(savefile, 'wb') as handle:\n",
        "         pickle.dump(dic, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  return dic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDguayUVL74j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute the above function\n",
        "savefile = '/content/drive/My Drive/mosi_data/text_data_joined.pickle'\n",
        "dic = get_text_data_joined(mypath,files,savefile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouoDWPRMk1sR",
        "colab_type": "text"
      },
      "source": [
        "# **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKs_4-TrMmAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the processed transcripts and the labels\n",
        "import pickle\n",
        "with open('/content/drive/My Drive/mosi_data/labels_joined.pickle', 'rb') as handle:\n",
        "    label= pickle.load(handle)\n",
        "\n",
        "with open('/content/drive/My Drive/mosi_data/text_data_joined.pickle', 'rb') as handle:\n",
        "    dic = pickle.load(handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrax9H42Ndrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Join all the segments of the text data into a numpy array\n",
        "import numpy as np\n",
        "review = []\n",
        "for key in files:\n",
        "  review+=dic[key]\n",
        "review = np.array(review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QB1pvNWOJBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Join all the labels into a numpy array\n",
        "import numpy as np\n",
        "y = []\n",
        "for key in files:\n",
        "    y+=label[key]\n",
        "y = np.array(y)\n",
        "\n",
        "#y[y>0]=1        #Convert labels to binary\n",
        "#y[y<0]=0\n",
        "\n",
        "y=y.astype(int)   # Execute this line for classification. Comment it for regression\n",
        "ref = {-3:0,-2:1,-1:2,0:3,1:4,2:5,3:6}         #Uncomment the following three lines for 7 class classification\n",
        "for i,num in enumerate(y):\n",
        "  y[i] = ref[num]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCw9LAfZqifa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to generate train-test split. Arguments are text data,labels,audio features data and split_size (0.8 mean 80:20 train-test split)\n",
        "def split_data(text,labels,split_size=0.8):\n",
        "  import numpy as np  \n",
        "  train_length =int(len(labels)*split_size)\n",
        "  test_length =int(len(labels)-train_length)\n",
        "  idx = np.random.permutation(labels.shape[0])\n",
        "  text = text[idx]\n",
        "  labels = labels[idx]\n",
        "  text_train = text[:train_length]\n",
        "  text_val = text[train_length:]\n",
        "  labels_train = labels[:train_length]\n",
        "  labels_val = labels[train_length:]\n",
        "  \n",
        "  return text_train,text_val,labels_train,labels_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EfYZzqCsnrR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get train-test split\n",
        "train_reviews,  val_reviews, train_labels, val_labels = split_data(review,y,0.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXiv6wKKR4tk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the maximum sequence length of text data\n",
        "maximum = float('-inf')\n",
        "for s in review:\n",
        "    maximum = max(maximum,len(s))\n",
        "print(maximum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7_LJhsUNpMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocess the text data. Similar to the audio data, segments of the text data are paddded to have same length\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_length = 581\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "vocab_len=5000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_len+1,oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(review)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size=len(word_index)\n",
        "print('Size of Vocabulary: ',vocab_size)\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_reviews)\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "val_sequences = tokenizer.texts_to_sequences(val_reviews)\n",
        "val_padded = pad_sequences(val_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "train_labels=np.expand_dims(train_labels, axis=1)\n",
        "val_labels=np.expand_dims(val_labels, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK7-McSITMp2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Statistics\n",
        "print('Dimension of Training  Text Data: ',train_padded.shape)\n",
        "print('Dimension of Validation Text Data: ',val_padded.shape)\n",
        "print('Dimension of Training Labels: ',train_labels.shape)\n",
        "print('Dimension of Validation Labels: ',val_labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1VdIclHTXIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download Glove Wiki Embeddings\n",
        "!wget --no-check-certificate \\\n",
        "      \"http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\"\\\n",
        "      -O \"/content/drive/My Drive/mosi_data/globe6B.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvljIHkqUHiH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unzip the downloaded embeddings\n",
        "!unzip -q '/content/drive/My Drive/mosi_data/globe6B.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r03B041VDGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the embeddings. There are 4 dimensions to choose from. I used 300 dimensional embeddings. \n",
        "embeddings_index = {}\n",
        "with open('/content/glove.6B.300d.txt') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM6UeNbZVxiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Map the embeddings with the words of the text data\n",
        "embedding_dim = 300\n",
        "embeddings_matrix = np.zeros((vocab_size+1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embeddings_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWBwnMbjlJud",
        "colab_type": "text"
      },
      "source": [
        "#**Training** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBzay1SGhyWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to compute F1 score. I use it as a metrics for Binary Classification.\n",
        "from keras.callbacks import Callback,ModelCheckpoint\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import keras.backend as K\n",
        "def f1_score(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-_KSKoWize0",
        "colab_type": "text"
      },
      "source": [
        "## **Baseline**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSUC5RIBhLos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Provide the suitable units inside the Dense layer.\n",
        "# For Binary classification, use 1 and 'sigmoid' as activation\n",
        "# For 7 class classification, use 7 and 'softmax' as activation\n",
        "# For Regression, use 1 and remove activation\n",
        "\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# text only model\n",
        "inp = Input(max_length)   \n",
        "layer = tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights = [embeddings_matrix], trainable = False)(inp)            \n",
        "layer = tf.keras.layers.Dropout(0.4)(layer)\n",
        "layer = tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.LSTM(128,return_sequences=True))(layer)\n",
        "layer = tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.LSTM(128))(layer)\n",
        "layer = tf.keras.layers.Dropout(0.2)(layer)\n",
        "layer = tf.keras.layers.Dense(128, activation='relu')(layer)\n",
        "layer = tf.keras.layers.Dropout(0.4)(layer)\n",
        "layer = tf.keras.layers.Dense(64, activation='relu')(layer)\n",
        "layer = tf.keras.layers.Dropout(0.5)(layer)\n",
        "out = tf.keras.layers.Dense(1, activation='sigmoid')(layer) \n",
        "#out = tf.keras.layers.Dense(7, activation='softmax')(layer) \n",
        "#out = tf.keras.layers.Dense(1)(layer)                 \n",
        "model= Model(inp,out)                                 \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K7pB9tpWnv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reduce =tf. keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, mode='auto')  #to reduce learning rate by factor of 0.1 if model performance degrades for 10 (patience) epochs.  \n",
        "#early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=10, mode='auto')  #early stopping if performance of model degrades for 10 epochs\n",
        "\n",
        "#Uncomment one of the next three lines at a time\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy',f1_score])                #Binary classification\n",
        "#model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])            #7 class classifiaction\n",
        "#model.compile(loss=\"mean_absolute_error\",optimizer='adam',metrics=[\"mean_absolute_error\"])             #Regression\n",
        "\n",
        "# I am training for 50 epochs with a batch size of 256. Set verbose to 2 for no training details and 0 for more training details.\n",
        "num_epochs = 50\n",
        "history=model.fit(train_padded, \n",
        "                    train_labels, \n",
        "                    epochs=num_epochs, \n",
        "                    batch_size=256, \n",
        "                    validation_data=(val_padded,val_labels),\n",
        "                    callbacks=[reduce],\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y1aFWN-iov8",
        "colab_type": "text"
      },
      "source": [
        "## **Transformer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDamnIVACffG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class MultiHeadSelfAttention(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads=8):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        if embed_dim % num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
        "            )\n",
        "        self.projection_dim = embed_dim // num_heads\n",
        "        self.query_dense = layers.Dense(embed_dim)\n",
        "        self.key_dense = layers.Dense(embed_dim)\n",
        "        self.value_dense = layers.Dense(embed_dim)\n",
        "        self.combine_heads = layers.Dense(embed_dim)\n",
        "\n",
        "    def attention(self, query, key, value):\n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "        output = tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        query = self.separate_heads(\n",
        "            query, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        key = self.separate_heads(\n",
        "            key, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        value = self.separate_heads(\n",
        "            value, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        attention, weights = self.attention(query, key, value)\n",
        "        attention = tf.transpose(\n",
        "            attention, perm=[0, 2, 1, 3]\n",
        "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
        "        concat_attention = tf.reshape(\n",
        "            attention, (batch_size, -1, self.embed_dim)\n",
        "        )  # (batch_size, seq_len, embed_dim)\n",
        "        output = self.combine_heads(\n",
        "            concat_attention\n",
        "        )  # (batch_size, seq_len, embed_dim)\n",
        "        return output\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMGXyRl72amX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Transformer(layers.Layer):\n",
        "    def __init__(self, maxlen, embed_dim, vocab_size,embeddings_matrix,num_heads):\n",
        "        super(Transformer, self).__init__()\n",
        "        \n",
        "        self.embed = tf.keras.layers.Embedding(vocab_size+1, embed_dim,  input_length=maxlen, weights = [embeddings_matrix], trainable = False)\n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "\n",
        "        self.lstm1 = tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(128,return_sequences=True))\n",
        "        self.lstm2 = tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(128,return_sequences=True))\n",
        "        self.dropout1 = tf.keras.layers.Dropout(0.2)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(0.2)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(0.2)\n",
        "        self.dropout4 = tf.keras.layers.Dropout(0.4)\n",
        "        self.dropout5 = tf.keras.layers.Dropout(0.5)\n",
        "        self.dropout6 = tf.keras.layers.Dropout(0.4)\n",
        "        self.pool     =  tf.keras.layers.GlobalAveragePooling1D()\n",
        "        self.dense1 = tf.keras.layers.Dense(128, activation=\"relu\")\n",
        "        self.dense2 = tf.keras.layers.Dense(64, activation=\"relu\")\n",
        "        #self.out = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "        self.out = tf.keras.layers.Dense(7, activation=\"softmax\")\n",
        "        #self.out = tf.keras.layers.Dense(1)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        \n",
        "        inputs = self.embed(inputs) \n",
        "        inputs = self.dropout6(inputs)\n",
        "        attn_output = self.att(inputs)\n",
        "        x = inputs + attn_output\n",
        "        x = self.dropout1(x)\n",
        "        x = self.lstm1(x)\n",
        "        x = self.lstm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = self.dense1(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = self.dense2(x)\n",
        "        x = self.dropout4(x)\n",
        "        out = self.out(x)\n",
        "        \n",
        "        return out\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwxE8xil3Bo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 3108\n",
        "maxlen = 581\n",
        "embed_dim = 300  # Embedding size for each token\n",
        "num_heads = 10  # Number of attention heads\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "transformer_block = Transformer(maxlen, embed_dim, vocab_size,embeddings_matrix,num_heads)\n",
        "outputs = transformer_block(inputs)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LYZSHlc3ij9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reduce =tf. keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, mode='auto')\n",
        "#model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "#model.compile(optimizer=\"adam\", loss=\"mean_absolute_error\")\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(\n",
        "    train_padded, train_labels, batch_size=256, epochs=30, validation_data=(val_padded, val_labels),callbacks=[reduce]\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}