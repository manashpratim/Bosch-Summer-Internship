{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A+V+T_V3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOBEBbOssxNnLuJazhARMrg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manashpratim/Bosch-Summer-Internship/blob/master/A%2BV%2BT_V3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puL55onzXCsj",
        "colab_type": "text"
      },
      "source": [
        "### Data is available at https://drive.google.com/drive/folders/1NFYIaXjL8V5kvZo3g9JEafLQ3scslWic?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxiEtdN4XF6G",
        "colab_type": "text"
      },
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix_EEkG-W-Dd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2s-4ju0XOoF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unzip the MOSI data\n",
        "!unzip -q '/content/drive/My Drive/mosi_data/mosi.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3DZNP1EXOui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to extract file names\n",
        "def get_file_names(mypath,savefile):      \n",
        "  from os import listdir\n",
        "  from os.path import isfile, join\n",
        "  onlyfiles = [f[:f.find('.')] for f in listdir(mypath) if isfile(join(mypath, f))]\n",
        "  with open(savefile, 'w') as f:\n",
        "    for item in onlyfiles:\n",
        "        f.write(item)\n",
        "        f.write('\\n')\n",
        "  return onlyfiles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYhrhHvpXO5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the names of the files. I use the names of the Transcript files as all the other data formats are adjusted based on this format\n",
        "mypath = '/content/Raw/Transcript/Segmented'\n",
        "files = get_file_names(mypath,'textfile.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW9bKl9nXg7h",
        "colab_type": "text"
      },
      "source": [
        "## **Preprocessing Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzKM9aWxXPEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the labels and saved audio features\n",
        "import pickle\n",
        "with open('/content/drive/My Drive/mosi_data/audio_features_joined.pickle', 'rb') as handle:\n",
        "    dic = pickle.load(handle)\n",
        "with open('/content/drive/My Drive/mosi_data/video_features_dense.pickle', 'rb') as handle:\n",
        "    dic2 = pickle.load(handle)\n",
        "with open('/content/drive/My Drive/mosi_data/text_data_joined.pickle', 'rb') as handle:\n",
        "    dic3 = pickle.load(handle)\n",
        "with open('/content/drive/My Drive/mosi_data/labels_joined.pickle', 'rb') as handle:\n",
        "    labels = pickle.load(handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcVQ_b7dXPT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data Normalization\n",
        "def preprocessing(arr,flag=False):\n",
        "    mean =  np.mean(arr,axis=0)\n",
        "    std = np.std(arr,axis=0)\n",
        "    if flag:\n",
        "      arr = (arr-mean)/std\n",
        "    else:\n",
        "      arr = (arr-mean)\n",
        "    return arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SLIKRFIXPMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Text\n",
        "import numpy as np\n",
        "review = []\n",
        "for key in files:\n",
        "  review+=dic3[key]\n",
        "review = np.array(review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcosqDtzXPYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This block pads the audio features so that each segments have same length. I use zero padding\n",
        "audio_data = []\n",
        "for key in files:\n",
        "  for l in dic[key]:\n",
        "        l = preprocessing(l,flag=True)             \n",
        "        audio_data.append(l)\n",
        "\n",
        "import tensorflow as tf\n",
        "audio_data = tf.keras.preprocessing.sequence.pad_sequences(audio_data, maxlen=142, dtype='float32', padding='post', truncating='post',value=0.0)\n",
        "audio_data = audio_data[:,:,7:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdGkKpznXPQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This block pads the video features so that each segments have same length. I use zero padding\n",
        "video_data = []\n",
        "maximum = float('-inf')\n",
        "for key in files:\n",
        "  for l in dic2[key]:            \n",
        "        video_data.append(l)\n",
        "\n",
        "import tensorflow as tf\n",
        "video_data = tf.keras.preprocessing.sequence.pad_sequences(video_data, maxlen=142, dtype='float32', padding='post', truncating='post',value=0.0)\n",
        "video_data = video_data[:,:,:500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S54thM3pXPKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Join all the labels into a numpy array\n",
        "import numpy as np\n",
        "y = []\n",
        "for key in files:\n",
        "    y+=labels[key]\n",
        "y = np.array(y)\n",
        "\n",
        "#For regression, do not execute any of the lines below\n",
        "\n",
        "y[y>0]=1        #Convert labels to binary\n",
        "y[y<0]=0\n",
        "\n",
        "y=y.astype(int)   \n",
        "\n",
        "#ref = {-3:0,-2:1,-1:2,0:3,1:4,2:5,3:6}         #Uncomment the following three lines for 7 class classification\n",
        "#for i,num in enumerate(y):\n",
        "#  y[i] = ref[num]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1cPDpwjXPIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to generate train-test split. Arguments are text data,labels,audio features data and split_size (0.8 mean 80:20 train-test split)\n",
        "def split_data(audio,video,text,labels,split_size=0.8):\n",
        "  import numpy as np  \n",
        "  train_length =int(len(labels)*split_size)\n",
        "  test_length =int(len(labels)-train_length)\n",
        "  idx = np.random.permutation(labels.shape[0])\n",
        "  audio = audio[idx]\n",
        "  labels = labels[idx]\n",
        "  video = video[idx]\n",
        "  text = text[idx]\n",
        "  audio_train = audio[:train_length]\n",
        "  audio_val = audio[train_length:]\n",
        "  video_train = video[:train_length]\n",
        "  video_val = video[train_length:]\n",
        "  text_train = text[:train_length]\n",
        "  text_val = text[train_length:]\n",
        "  labels_train = labels[:train_length]\n",
        "  labels_val = labels[train_length:]\n",
        "  \n",
        "  return audio_train,audio_val,video_train,video_val,text_train,text_val,labels_train,labels_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Luv1d2lGXPBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "audio_train,audio_val,video_train,video_val,train_reviews,val_reviews,labels_train,labels_val = split_data(audio_data,video_data,review,y,split_size=0.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeZPhIjQXO_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_length = 142\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "vocab_len=5000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_len+1,oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(review)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size=len(word_index)\n",
        "print('Size of Vocabulary: ',vocab_size)\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_reviews)\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "val_sequences = tokenizer.texts_to_sequences(val_reviews)\n",
        "val_padded = pad_sequences(val_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "labels_train=np.expand_dims(labels_train, axis=1)\n",
        "labels_val=np.expand_dims(labels_val, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EYtPFBpXO9T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q '/content/drive/My Drive/mosi_data/globe6B.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaGwYTk2XO2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_index = {}\n",
        "with open('/content/glove.6B.300d.txt') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME_a6lLeXO0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 300\n",
        "embeddings_matrix = np.zeros((vocab_size+1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embeddings_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZ5NJ_1IXOyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Statistics\n",
        "print('Dimension of Training  Audio Data: ',audio_train.shape)\n",
        "print('Dimension of Validation Audio Data: ',audio_val.shape)\n",
        "print('Dimension of Training  Video Data: ',video_train.shape)\n",
        "print('Dimension of Validation Video Data: ',video_val.shape)\n",
        "print('Dimension of Training  Text Data: ',train_padded.shape)\n",
        "print('Dimension of Validation Text Data: ',val_padded.shape)\n",
        "print('Dimension of Training Labels: ',labels_train.shape)\n",
        "print('Dimension of Validation Labels: ',labels_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "815s_LsgY71w",
        "colab_type": "text"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgrp9Y6iY3Fc",
        "colab_type": "text"
      },
      "source": [
        "### **Early Fusion using Transformers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUTYwbZxXOrU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class MultiHeadSelfAttention(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads=8):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        if embed_dim % num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
        "            )\n",
        "        self.projection_dim = embed_dim // num_heads\n",
        "        self.query_dense = layers.Dense(embed_dim)\n",
        "        self.key_dense = layers.Dense(embed_dim)\n",
        "        self.value_dense = layers.Dense(embed_dim)\n",
        "        self.combine_heads = layers.Dense(embed_dim)\n",
        "\n",
        "    def attention(self, query, key, value):\n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "        output = tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        query = self.separate_heads(\n",
        "            query, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        key = self.separate_heads(\n",
        "            key, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        value = self.separate_heads(\n",
        "            value, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        attention, weights = self.attention(query, key, value)\n",
        "        attention = tf.transpose(\n",
        "            attention, perm=[0, 2, 1, 3]\n",
        "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
        "        concat_attention = tf.reshape(\n",
        "            attention, (batch_size, -1, self.embed_dim)\n",
        "        )  # (batch_size, seq_len, embed_dim)\n",
        "        output = self.combine_heads(\n",
        "            concat_attention\n",
        "        )  # (batch_size, seq_len, embed_dim)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGz7_VoJZB7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(layers.Layer):\n",
        "    def __init__(self, maxlen, embed_dim,num_heads):\n",
        "        super(Transformer, self).__init__()\n",
        "        \n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.lstm1 = tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(128,return_sequences=True))\n",
        "        self.lstm2 = tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(128,return_sequences=True))\n",
        "        self.dropout1 = tf.keras.layers.Dropout(0.2)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(0.2)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(0.2)\n",
        "        self.dropout4 = tf.keras.layers.Dropout(0.4)\n",
        "        self.dropout5 = tf.keras.layers.Dropout(0.5)\n",
        "\n",
        "        self.pool     =  tf.keras.layers.GlobalAveragePooling1D()\n",
        "        self.dense1 = tf.keras.layers.Dense(128, activation=\"relu\",kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-3, l2=1e-2))\n",
        "        self.dense2 = tf.keras.layers.Dense(64, activation=\"relu\",kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-2, l2=1e-2))\n",
        "        self.out = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "        #self.out = tf.keras.layers.Dense(7, activation=\"softmax\")\n",
        "        #self.out = tf.keras.layers.Dense(1)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "\n",
        "        attn_output = self.att(inputs)\n",
        "        x = inputs + attn_output\n",
        "        x = self.dropout1(x)\n",
        "        \n",
        "        x = self.lstm1(x)\n",
        "        x = self.lstm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        \n",
        "        x = self.pool(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = self.dense1(x)\n",
        "        x = self.dropout4(x)\n",
        "        x = self.dense2(x)\n",
        "        x = self.dropout5(x)\n",
        "        out = self.out(x)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMdImc4cbD7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "audio_video_train = np.concatenate((audio_train,video_train),axis=-1)\n",
        "audio_video_val = np.concatenate((audio_val,video_val),axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdBADB5ybEdS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlen = 142\n",
        "num_heads = 2\n",
        "\n",
        "inputs1 = layers.Input(shape=(maxlen,))\n",
        "embed = tf.keras.layers.Embedding(3108+1, 300,  input_length=maxlen, weights = [embeddings_matrix], trainable = False)(inputs1)\n",
        "embed = tf.keras.layers.Dropout(0.4)(embed)\n",
        "model1 = keras.Model(inputs=inputs1, outputs=embed)\n",
        "\n",
        "inputs2 = layers.Input(shape=(maxlen,550))\n",
        "\n",
        "fusion = keras.layers.Concatenate(axis=2)([model1.output, inputs2])\n",
        "transformer_block = Transformer(maxlen,850,num_heads)\n",
        "outputs = transformer_block(fusion)\n",
        "model = keras.Model(inputs=[inputs1,inputs2], outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioV18eg1bpVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reduce =tf. keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, mode='auto')\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "#model.compile(optimizer=\"adam\", loss=\"mean_absolute_error\")\n",
        "#model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "#history = model.fit(\n",
        "#    audio_train, labels_train, batch_size=256, epochs=80, validation_data=(audio_val, labels_val),callbacks=[reduce]\n",
        "#)\n",
        "history = model.fit(\n",
        "    [train_padded,audio_video_train], labels_train, batch_size=32, epochs=80, shuffle=True,validation_data=([val_padded,audio_video_val], labels_val),callbacks=[reduce]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}