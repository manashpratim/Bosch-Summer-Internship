{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A+V+T_V4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOjgBAT+cH8x2vDl+z8KPVc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manashpratim/Bosch-Summer-Internship/blob/master/A%2BV%2BT_V4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fq3oYFt1XWDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4BnHq0XX8Jw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "      \"http://immortal.multicomp.cs.cmu.edu/raw_datasets/CMU_MOSI.zip\"\\\n",
        "      -O \"/content/mosi.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YowSGJb4YBgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Unzip the MOSI data\n",
        "!unzip -q '/content/mosi.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_IIjWilYU16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install adversarial-robustness-toolbox"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXF8tTkMYVqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYDZst5wX8QJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to extract file names\n",
        "def get_file_names(mypath,savefile):      \n",
        "  from os import listdir\n",
        "  from os.path import isfile, join\n",
        "  onlyfiles = [f[:f.find('.')] for f in listdir(mypath) if isfile(join(mypath, f))]\n",
        "  with open(savefile, 'w') as f:\n",
        "    for item in onlyfiles:\n",
        "        f.write(item)\n",
        "        f.write('\\n')\n",
        "  return onlyfiles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAo4F1SWX8cH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the names of the files. I use the names of the Transcript files as all the other data formats are adjusted based on this format\n",
        "mypath = '/content/Raw/Transcript/Segmented'\n",
        "files = get_file_names(mypath,'textfile.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhtFGl12X8ns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the labels and saved audio features\n",
        "import pickle\n",
        "with open('/content/drive/My Drive/mosi_data/audio_features_joined.pickle', 'rb') as handle:\n",
        "    dic = pickle.load(handle)\n",
        "with open('/content/drive/My Drive/mosi_data/video_features_dense.pickle', 'rb') as handle:\n",
        "    dic2 = pickle.load(handle)\n",
        "with open('/content/drive/My Drive/mosi_data/text_data_joined.pickle', 'rb') as handle:\n",
        "    dic3 = pickle.load(handle)\n",
        "with open('/content/drive/My Drive/mosi_data/labels_joined.pickle', 'rb') as handle:\n",
        "    labels = pickle.load(handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkY3OkIAX8uP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data normalization\n",
        "def preprocessing(arr,flag=False):\n",
        "    mean =  np.mean(arr,axis=0)\n",
        "    std = np.std(arr,axis=0)\n",
        "    if flag:\n",
        "      arr = (arr-mean)/std\n",
        "    else:\n",
        "      arr = (arr-mean)\n",
        "    return arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeQDjmIxX8zr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Text\n",
        "import numpy as np\n",
        "review = []\n",
        "for key in files:\n",
        "  review+=dic3[key]\n",
        "review = np.array(review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMMx20ZcZSnM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This block pads the audio features so that each segments have same length. I use zero padding\n",
        "audio_data = []\n",
        "for key in files:\n",
        "  for l in dic[key]:\n",
        "        l = preprocessing(l,flag=True)             \n",
        "        audio_data.append(l)\n",
        "\n",
        "import tensorflow as tf\n",
        "audio_data = tf.keras.preprocessing.sequence.pad_sequences(audio_data, maxlen=1639, dtype='float32', padding='post', truncating='post',value=0.0)\n",
        "audio_data = audio_data[:,:,7:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ed2HqKVPX8sD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This block pads the video features so that each segments have same length. I use zero padding\n",
        "video_data = []\n",
        "for key in files:\n",
        "  for l in dic2[key]:            \n",
        "      video_data.append(l)\n",
        "\n",
        "import tensorflow as tf\n",
        "video_data = tf.keras.preprocessing.sequence.pad_sequences(video_data, maxlen=142, dtype='float32', padding='post', truncating='post',value=0.0)\n",
        "video_data = video_data[:,:,:500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEqnB7JMX8lO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Join all the labels into a numpy array\n",
        "import numpy as np\n",
        "y = []\n",
        "for key in files:\n",
        "    y+=labels[key]\n",
        "y = np.array(y)\n",
        "\n",
        "#For regression, do not execute any of the lines below\n",
        "\n",
        "y[y>0]=1        #Convert labels to binary\n",
        "y[y<0]=0\n",
        "\n",
        "y=y.astype(int)   \n",
        "\n",
        "#ref = {-3:0,-2:1,-1:2,0:3,1:4,2:5,3:6}         #Uncomment the following three lines for 7 class classification\n",
        "#for i,num in enumerate(y):\n",
        "#  y[i] = ref[num]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybAUw6njX8i9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to generate train-test split. Arguments are text data,labels,audio features data and split_size (0.8 mean 80:20 train-test split)\n",
        "def split_data(audio,video,text,labels,split_size=0.8):\n",
        "  import numpy as np  \n",
        "  train_length =int(len(labels)*split_size)\n",
        "  test_length =int(len(labels)-train_length)\n",
        "  idx = np.random.permutation(labels.shape[0])\n",
        "  audio = audio[idx]\n",
        "  labels = labels[idx]\n",
        "  video = video[idx]\n",
        "  text = text[idx]\n",
        "  audio_train = audio[:train_length]\n",
        "  audio_val = audio[train_length:]\n",
        "  video_train = video[:train_length]\n",
        "  video_val = video[train_length:]\n",
        "  text_train = text[:train_length]\n",
        "  text_val = text[train_length:]\n",
        "  labels_train = labels[:train_length]\n",
        "  labels_val = labels[train_length:]\n",
        "  \n",
        "  return audio_train,audio_val,video_train,video_val,text_train,text_val,labels_train,labels_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcrF8R9QX8g0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "audio_train,audio_val,video_train,video_val,train_reviews,val_reviews,labels_train,labels_val = split_data(audio_data,video_data,review,y,split_size=0.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfinjGlVX8Z0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_length = 581\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "vocab_len=5000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_len+1,oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(review)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size=len(word_index)\n",
        "print('Size of Vocabulary: ',vocab_size)\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_reviews)\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "val_sequences = tokenizer.texts_to_sequences(val_reviews)\n",
        "val_padded = pad_sequences(val_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "labels_train=np.expand_dims(labels_train, axis=1)\n",
        "labels_val=np.expand_dims(labels_val, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi0Ja8bSX8X1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q '/content/drive/My Drive/mosi_data/globe6B.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSJh-RgmX8VR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_index = {}\n",
        "with open('/content/glove.6B.300d.txt') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xe9sLx9X8Nw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "embedding_dim = 300\n",
        "embeddings_matrix = np.zeros((vocab_size+1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embeddings_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vqRODLUZzDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Statistics\n",
        "print('Dimension of Training  Audio Data: ',audio_train.shape)\n",
        "print('Dimension of Validation Audio Data: ',audio_val.shape)\n",
        "print('Dimension of Training  Video Data: ',video_train.shape)\n",
        "print('Dimension of Validation Video Data: ',video_val.shape)\n",
        "print('Dimension of Training  Text Data: ',train_padded.shape)\n",
        "print('Dimension of Validation Text Data: ',val_padded.shape)\n",
        "print('Dimension of Training Labels: ',labels_train.shape)\n",
        "print('Dimension of Validation Labels: ',labels_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANW4jj8PF5Vg",
        "colab_type": "text"
      },
      "source": [
        "## **Late Fusion (Weighted Sum of Logits)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cragz0jeZy_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class MultiHeadSelfAttention(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads=8):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        if embed_dim % num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
        "            )\n",
        "        self.projection_dim = embed_dim // num_heads\n",
        "        self.query_dense = layers.Dense(embed_dim)\n",
        "        self.key_dense = layers.Dense(embed_dim)\n",
        "        self.value_dense = layers.Dense(embed_dim)\n",
        "        self.combine_heads = layers.Dense(embed_dim)\n",
        "\n",
        "    def attention(self, query, key, value):\n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "        output = tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        query = self.separate_heads(\n",
        "            query, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        key = self.separate_heads(\n",
        "            key, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        value = self.separate_heads(\n",
        "            value, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        attention, weights = self.attention(query, key, value)\n",
        "        attention = tf.transpose(\n",
        "            attention, perm=[0, 2, 1, 3]\n",
        "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
        "        concat_attention = tf.reshape(\n",
        "            attention, (batch_size, -1, self.embed_dim)\n",
        "        )  # (batch_size, seq_len, embed_dim)\n",
        "        output = self.combine_heads(\n",
        "            concat_attention\n",
        "        )  # (batch_size, seq_len, embed_dim)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp42xuhfZy7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Audio model\n",
        "class Transformer1(layers.Layer):\n",
        "    def __init__(self, maxlen, embed_dim,num_heads):\n",
        "        super(Transformer1, self).__init__()\n",
        "        \n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(64, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(0.3)\n",
        "        self.dropout2 = layers.Dropout(0.5)\n",
        "        self.dropout3 = layers.Dropout(0.5)\n",
        "        self.pool = tf.keras.layers.GlobalAveragePooling1D()\n",
        "        self.dense = tf.keras.layers.Dense(64, activation=\"relu\")\n",
        "        self.out = tf.keras.layers.Dense(2)\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        attn_output = self.att(inputs)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        out = self.dropout2(ffn_output)\n",
        "        \n",
        "        out =  self.layernorm2(out1 + ffn_output)\n",
        "        out =  self.pool(out)\n",
        "        out = self.dense(out)\n",
        "        return self.out(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxpr2FqVZy3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Video Model\n",
        "class Transformer2(layers.Layer):\n",
        "    def __init__(self, maxlen, embed_dim,num_heads):\n",
        "        super(Transformer2, self).__init__()\n",
        "        \n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "\n",
        "        self.lstm1 = tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(128,return_sequences=True))\n",
        "        self.lstm2 = tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(128,return_sequences=True))\n",
        "        self.dropout1 = tf.keras.layers.Dropout(0.2)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(0.2)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(0.2)\n",
        "        self.dropout4 = tf.keras.layers.Dropout(0.4)\n",
        "        self.dropout5 = tf.keras.layers.Dropout(0.5)\n",
        "\n",
        "        self.pool     =  tf.keras.layers.GlobalAveragePooling1D()\n",
        "\n",
        "        self.dense1 = tf.keras.layers.Dense(128, activation=\"relu\")\n",
        "        self.dense2 = tf.keras.layers.Dense(64, activation=\"relu\")\n",
        "        self.out = tf.keras.layers.Dense(2)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        \n",
        "        attn_output = self.att(inputs)\n",
        "        x = inputs + attn_output\n",
        "        x = self.dropout1(x)\n",
        "        x = self.lstm1(x)\n",
        "        x = self.lstm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = self.dense1(x)\n",
        "        x = self.dropout4(x)\n",
        "        x = self.dense2(x)\n",
        "   \n",
        "        return self.out(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsQqzGfiZyzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Text Model\n",
        "class Transformer3(layers.Layer):\n",
        "    def __init__(self, maxlen, embed_dim, vocab_size,embeddings_matrix,num_heads):\n",
        "        super(Transformer3, self).__init__()\n",
        "        \n",
        "        self.embed = tf.keras.layers.Embedding(vocab_size+1, embed_dim,  input_length=maxlen, weights = [embeddings_matrix], trainable = False)\n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "\n",
        "        self.lstm1 = tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(128,return_sequences=True))\n",
        "        self.lstm2 = tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(128,return_sequences=True))\n",
        "        self.dropout1 = tf.keras.layers.Dropout(0.2)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(0.2)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(0.2)\n",
        "        self.dropout4 = tf.keras.layers.Dropout(0.4)\n",
        "        self.dropout5 = tf.keras.layers.Dropout(0.5)\n",
        "        self.dropout6 = tf.keras.layers.Dropout(0.4)\n",
        "        self.pool     =  tf.keras.layers.GlobalAveragePooling1D()\n",
        "        self.dense1 = tf.keras.layers.Dense(128, activation=\"relu\")\n",
        "        self.dense2 = tf.keras.layers.Dense(64, activation=\"relu\")\n",
        "        self.out = tf.keras.layers.Dense(2)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        \n",
        "        inputs = self.embed(inputs) \n",
        "        inputs = self.dropout6(inputs)\n",
        "        attn_output = self.att(inputs)\n",
        "        x = inputs + attn_output\n",
        "        x = self.dropout1(x)\n",
        "        x = self.lstm1(x)\n",
        "        x = self.lstm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = self.dense1(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = self.dense2(x)\n",
        "        \n",
        "        return self.out(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV9tDPymX8HT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Customized layer for weighted sum\n",
        "class WeightedSum(tf.keras.layers.Layer):\n",
        "    def __init__(self, a,b,c, **kwargs):\n",
        "        self.a = a\n",
        "        self.b = b\n",
        "        self.c = c\n",
        "        super(WeightedSum, self).__init__(**kwargs)\n",
        "    def call(self, model_outputs):\n",
        "        return self.a * model_outputs[0] + self.b * model_outputs[1] + self.c * model_outputs[2]\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HtCNrDhaF40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "num_heads = 5  # Number of attention heads\n",
        "\n",
        "inputs1 = layers.Input(shape=(1639,50))\n",
        "transformer_block = Transformer1(1639, 50,num_heads)\n",
        "outputs1 = transformer_block(inputs1)\n",
        "model1 = keras.Model(inputs=inputs1, outputs=outputs1)\n",
        "\n",
        "inputs2 = layers.Input(shape=(142,500))\n",
        "transformer_block1 = Transformer2(142, 500,num_heads)\n",
        "outputs2 = transformer_block1(inputs2)\n",
        "model2 = keras.Model(inputs=inputs2, outputs=outputs2)\n",
        "\n",
        "inputs3 = layers.Input(shape=(581,))\n",
        "transformer_block3 = Transformer3(581, 300, 3108,embeddings_matrix,num_heads)\n",
        "outputs3 = transformer_block3(inputs3)\n",
        "model3 = keras.Model(inputs=inputs3, outputs=outputs3)\n",
        "\n",
        "fusion = WeightedSum(0.2,0.3,0.5)([model1.output, model2.output,model3.output]) \n",
        "out = tf.keras.layers.Activation('softmax')(fusion)   \n",
        "model = keras.Model([model1.input,model2.input,model3.input],out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g7oqeglaF0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reduce =tf. keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=10, mode='auto')\n",
        "early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=12, mode='max',restore_best_weights=True)\n",
        "\n",
        "#Uncomment one of the next three lines at a time\n",
        "#model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])                #Binary classification\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])            #7 class classifiaction\n",
        "#model.compile(loss=\"mean_absolute_error\",optimizer='adam')             #Regression\n",
        "history=model.fit([audio_train,video_train,train_padded],labels_train, batch_size=32, epochs=50,\n",
        "                  #validation_data=(audio_val,labels_val),\n",
        "                  validation_split=0.1,\n",
        "                  shuffle = True,\n",
        "                  callbacks=[reduce,early])\n",
        "\n",
        "\n",
        "#model_save_filename = \"audio_model.h5\"\n",
        "\n",
        "#earlystopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
        "#mdlcheckpoint_cb = keras.callbacks.ModelCheckpoint(\n",
        "#    model_save_filename, monitor=\"val_accuracy\", save_best_only=True\n",
        "#)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2TK0gx2aFvk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = model.evaluate([audio_val,video_val,val_padded],labels_val)\n",
        "print('Test Set Performance: ',results)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}