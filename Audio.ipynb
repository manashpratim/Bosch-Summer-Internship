{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Audio.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRwtP-cvfIhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data is available at https://drive.google.com/drive/folders/1NFYIaXjL8V5kvZo3g9JEafLQ3scslWic?usp=sharing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUWj0hFqvvrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceRMsG2dzdxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mX1tmvyvk-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unzip the MOSI data\n",
        "!unzip -q '/content/drive/My Drive/mosi_data/mosi.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY5UhJ-EjsU7",
        "colab_type": "text"
      },
      "source": [
        "# **Audio Features Extraction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj4Rlo1-jy0Q",
        "colab_type": "text"
      },
      "source": [
        "The extracted features are available at the above link"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lKTa5FIwReB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to extract file names\n",
        "def get_file_names(mypath,savefile):      \n",
        "  from os import listdir\n",
        "  from os.path import isfile, join\n",
        "  onlyfiles = [f[:f.find('.')] for f in listdir(mypath) if isfile(join(mypath, f))]\n",
        "  with open(savefile, 'w') as f:\n",
        "    for item in onlyfiles:\n",
        "        f.write(item)\n",
        "        f.write('\\n')\n",
        "  return onlyfiles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJjEJWCmwdvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mypath = '/content/Raw/Audio/WAV_16000/Segmented' \n",
        "savefile = 'audiofiles.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmJlJVTfw4WF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "audiofiles = get_file_names(mypath,savefile)          #Get the names of the audio files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdrrVS5axTzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to extract audio features. Arguments are path of the audio data and the names of the audiofiles \n",
        "def get_audio_features(mypath,audiofiles):\n",
        "    import librosa\n",
        "    import numpy as np\n",
        "    mydic = {}\n",
        "    for file in audiofiles:\n",
        "      audiofile = mypath + '/' + file + '.wav'\n",
        "      x, sr = librosa.load(audiofile, sr  = 16000, res_type='kaiser_fast')        #Sampling frequency 16KHz\n",
        "      chroma_stft = librosa.feature.chroma_stft(y=x, sr=sr).T\n",
        "      rmse = librosa.feature.rmse(y=x).T\n",
        "      spec_cent = librosa.feature.spectral_centroid(y=x, sr=sr).T\n",
        "      spec_bw = librosa.feature.spectral_bandwidth(y=x, sr=sr).T\n",
        "      rolloff = librosa.feature.spectral_rolloff(y=x, sr=sr).T\n",
        "      zcr = librosa.feature.zero_crossing_rate(x).T\n",
        "      mfcc = librosa.feature.mfcc(y=x, sr=sr,n_mfcc=40).T                         #40 MFCC features\n",
        "      mydic[file] = np.hstack((chroma_stft,rmse,spec_cent,spec_bw,rolloff,zcr,mfcc))\n",
        "      mydic[file] = mfcc\n",
        "    return mydic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbXB5hGF25_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "audio = get_audio_features(mypath,audiofiles)                 #Get audio features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vArwXe_9P1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This block changes the format of the audio features data to match the other data modalities\n",
        "\n",
        "dic = {}\n",
        "for file in audiofiles:\n",
        "  new = file[:-file[::-1].find('_')-1]\n",
        "  dic[file] = new\n",
        "\n",
        "newdic = {}\n",
        "newdic1 = {}\n",
        "for key in audio:\n",
        "  newkey = dic[key]\n",
        "  if newkey not in newdic:\n",
        "    newdic[newkey] = {}\n",
        "  if newkey not in newdic1:\n",
        "    newdic1[newkey] = {}\n",
        "  newdic1[newkey][key] = audio[key]\n",
        "  k = key[-key[::-1].find('_'):]\n",
        "  newdic[newkey][int(k)] = audio[key]\n",
        "\n",
        "nd = {}\n",
        "for key in newdic:\n",
        "    nd[key] = []\n",
        "    for k in sorted(newdic[key].keys()):\n",
        "      nd[key].append(newdic[key][k])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Felu0HwKgTCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save the audio features data in pickle format\n",
        "import pickle\n",
        "with open('/content/drive/My Drive/mosi_data/audio_features_joined.pickle', 'wb') as handle:\n",
        "    pickle.dump(nd, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zotfpd_nj9g-",
        "colab_type": "text"
      },
      "source": [
        "# **Preprocessing the features and labels**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrgWwVaL-cA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the labels and saved audio features\n",
        "import pickle\n",
        "with open('/content/drive/My Drive/mosi_data/audio_features_joined.pickle', 'rb') as handle:\n",
        "    dic2 = pickle.load(handle)\n",
        "\n",
        "with open('/content/drive/My Drive/mosi_data/labels_joined.pickle', 'rb') as handle:\n",
        "    labels = pickle.load(handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIzl7b4IgDIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the names of the files. I use the names of the Transcript files as all the other data formats are adjusted based on this format\n",
        "mypath = '/content/Raw/Transcript/Segmented'\n",
        "files = get_file_names(mypath,'textfile.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jE47akLot3vB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This block pads the audio features so that each segments have same length. I use zero padding\n",
        "audio_data = []\n",
        "maximum = float('-inf')\n",
        "max_pad_len = 1639                            #max length of a sequence. For audio_features_joined (MFCC), use this\n",
        "#max_pad_len = 858                            #For audio_pretrained_features_joined (VGGish), use this. Uncomment the above\n",
        "\n",
        "for key in files:\n",
        "  for l in dic2[key]:\n",
        "\n",
        "    if len(l)>0:\n",
        "      #maximum = max(maximum,l.shape[0])\n",
        "      pad_width = max_pad_len - l.shape[0]\n",
        "      mfcc = np.pad(l.T, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "      audio_data.append(mfcc)\n",
        "    else:                                     # This else statement is for the VGGish features data. They have 28 bad frames. It does not affect the MFCC data\n",
        "      f = np.random.rand(128,858)               \n",
        "      audio_data.append(f)\n",
        "\n",
        "audio_data = np.array(audio_data)\n",
        "audio_data= audio_data.reshape(audio_data.shape[0], 57, 1639)             # For audio_features_joined\n",
        "#audio_data= audio_data.reshape(audio_data.shape[0], 128, 858)            # For audio_pretrained_features_joined"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynhyw_jSu9bT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Join all the labels into a numpy array\n",
        "import numpy as np\n",
        "y = []\n",
        "for key in files:\n",
        "    y+=labels[key]\n",
        "y = np.array(y)\n",
        "\n",
        "#For regression, do not execute any of the lines below\n",
        "\n",
        "y[y>0]=1        #Convert labels to binary\n",
        "y[y<0]=0\n",
        "\n",
        "y=y.astype(int)   \n",
        "\n",
        "#ref = {-3:0,-2:1,-1:2,0:3,1:4,2:5,3:6}         #Uncomment the following three lines for 7 class classification\n",
        "#for i,num in enumerate(y):\n",
        "#  y[i] = ref[num]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD33ZJVFkTAu",
        "colab_type": "text"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15rESw_K4lVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# audio model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "inp = Input((57,1639))           # Dimensions for MFCC data. For VGGish, change it to (128,858)\n",
        "layer = tf.keras.layers.Conv1D(64,3,activation='relu',input_shape=(57,1639))(inp)\n",
        "layer = tf.keras.layers.Conv1D(128,3,activation='relu')(layer)\n",
        "layer = tf.keras.layers.MaxPool1D(2)(layer)\n",
        "layer = tf.keras.layers.Conv1D(128,3,activation='relu',padding='same')(layer)\n",
        "layer = tf.keras.layers.Conv1D(256,3,activation='relu')(layer)\n",
        "layer = tf.keras.layers.MaxPool1D(2)(layer)\n",
        "layer = tf.keras.layers.Dropout(0.3)(layer)\n",
        "layer = tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(128,return_sequences=True))(layer)\n",
        "layer = tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(128))(layer)\n",
        "layer = tf.keras.layers.Dropout(0.2)(layer)\n",
        "layer = tf.keras.layers.Dense(256,activation='relu')(layer)\n",
        "layer = tf.keras.layers.Dropout(0.2)(layer)\n",
        "layer = tf.keras.layers.Dense(64,activation='relu')(layer)\n",
        "#layer = tf.keras.layers.Dropout(0.5)(layer)\n",
        "out = tf.keras.layers.Dense(1,activation='sigmoid')(layer)\n",
        "#out = tf.keras.layers.Dense(7,activation='softmax')(layer)\n",
        "#out = tf.keras.layers.Dense(1)(layer)\n",
        "model = Model(inp,out)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1GId9OIx8RP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reduce =tf. keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, mode='auto')\n",
        "#early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=10, mode='auto')\n",
        "\n",
        "#Uncomment one of the next three lines at a time\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])                #Binary classification\n",
        "#model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])            #7 class classifiaction\n",
        "#model.compile(loss=\"mean_absolute_error\",optimizer='adam',metrics=[\"mean_absolute_error\"])             #Regression\n",
        "history=model.fit(audio_data,y, batch_size=256, epochs=50, validation_split=0.2)#,callbacks=[reduce,early])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}