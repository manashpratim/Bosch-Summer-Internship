{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AudioV2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOlL/X4yS4KXEazNMWW+IKw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manashpratim/Bosch-Summer-Internship/blob/master/AudioV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRwtP-cvfIhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data is available at https://drive.google.com/drive/folders/1NFYIaXjL8V5kvZo3g9JEafLQ3scslWic?usp=sharing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUWj0hFqvvrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceRMsG2dzdxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mX1tmvyvk-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unzip the MOSI data\n",
        "!unzip -q '/content/drive/My Drive/mosi_data/mosi.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY5UhJ-EjsU7",
        "colab_type": "text"
      },
      "source": [
        "# **Audio Features Extraction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj4Rlo1-jy0Q",
        "colab_type": "text"
      },
      "source": [
        "The extracted features are available at the above link"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lKTa5FIwReB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to extract file names\n",
        "def get_file_names(mypath,savefile):      \n",
        "  from os import listdir\n",
        "  from os.path import isfile, join\n",
        "  onlyfiles = [f[:f.find('.')] for f in listdir(mypath) if isfile(join(mypath, f))]\n",
        "  with open(savefile, 'w') as f:\n",
        "    for item in onlyfiles:\n",
        "        f.write(item)\n",
        "        f.write('\\n')\n",
        "  return onlyfiles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJjEJWCmwdvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mypath = '/content/Raw/Audio/WAV_16000/Segmented' \n",
        "savefile = 'audiofiles.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmJlJVTfw4WF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "audiofiles = get_file_names(mypath,savefile)          #Get the names of the audio files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdrrVS5axTzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to extract audio features. Arguments are path of the audio data and the names of the audiofiles \n",
        "def get_audio_features(mypath,audiofiles):\n",
        "    import librosa\n",
        "    import numpy as np\n",
        "    mydic = {}\n",
        "    for file in audiofiles:\n",
        "      audiofile = mypath + '/' + file + '.wav'\n",
        "      x, sr = librosa.load(audiofile, sr  = 16000, res_type='kaiser_fast')        #Sampling frequency 16KHz\n",
        "      chroma_stft = librosa.feature.chroma_stft(y=x, sr=sr).T\n",
        "      rmse = librosa.feature.rmse(y=x).T\n",
        "      spec_cent = librosa.feature.spectral_centroid(y=x, sr=sr).T\n",
        "      spec_bw = librosa.feature.spectral_bandwidth(y=x, sr=sr).T\n",
        "      rolloff = librosa.feature.spectral_rolloff(y=x, sr=sr).T\n",
        "      zcr = librosa.feature.zero_crossing_rate(x).T\n",
        "      mfcc = librosa.feature.mfcc(y=x, sr=sr,n_mfcc=40).T                         #40 MFCC features\n",
        "      mydic[file] = np.hstack((chroma_stft,rmse,spec_cent,spec_bw,rolloff,zcr,mfcc))\n",
        "    return mydic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbXB5hGF25_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "audio = get_audio_features(mypath,audiofiles)                 #Get audio features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vArwXe_9P1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This block changes the format of the audio features data to match the other data modalities\n",
        "\n",
        "dic = {}\n",
        "for file in audiofiles:\n",
        "  new = file[:-file[::-1].find('_')-1]\n",
        "  dic[file] = new\n",
        "\n",
        "newdic = {}\n",
        "newdic1 = {}\n",
        "for key in audio:\n",
        "  newkey = dic[key]\n",
        "  if newkey not in newdic:\n",
        "    newdic[newkey] = {}\n",
        "  if newkey not in newdic1:\n",
        "    newdic1[newkey] = {}\n",
        "  newdic1[newkey][key] = audio[key]\n",
        "  k = key[-key[::-1].find('_'):]\n",
        "  newdic[newkey][int(k)] = audio[key]\n",
        "\n",
        "nd = {}\n",
        "for key in newdic:\n",
        "    nd[key] = []\n",
        "    for k in sorted(newdic[key].keys()):\n",
        "      nd[key].append(newdic[key][k])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Felu0HwKgTCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save the audio features data in pickle format\n",
        "import pickle\n",
        "with open('/content/drive/My Drive/mosi_data/audio_features_joined.pickle', 'wb') as handle:\n",
        "    pickle.dump(nd, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zotfpd_nj9g-",
        "colab_type": "text"
      },
      "source": [
        "# **Preprocessing the features and labels**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrgWwVaL-cA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the labels and saved audio features\n",
        "import pickle\n",
        "with open('/content/drive/My Drive/mosi_data/audio_features_joined.pickle', 'rb') as handle:\n",
        "    dic2 = pickle.load(handle)\n",
        "\n",
        "with open('/content/drive/My Drive/mosi_data/labels_joined.pickle', 'rb') as handle:\n",
        "    labels = pickle.load(handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIzl7b4IgDIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the names of the files. I use the names of the Transcript files as all the other data formats are adjusted based on this format\n",
        "mypath = '/content/Raw/Transcript/Segmented'\n",
        "files = get_file_names(mypath,'textfile.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck_8151QmMbL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing(arr,flag=False):\n",
        "    mean =  np.mean(arr,axis=0)\n",
        "    std = np.std(arr,axis=0)\n",
        "    if flag:\n",
        "      arr = (arr-mean)/std\n",
        "    else:\n",
        "      arr = (arr-mean)\n",
        "    return arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jE47akLot3vB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This block pads the audio features so that each segments have same length. I use zero padding\n",
        "audio_data = []\n",
        "maximum = float('-inf')\n",
        "max_pad_len = 1639                     #max length of a sequence. For audio_features_joined (MFCC), use this\n",
        "#max_pad_len = 858                            #For audio_pretrained_features_joined (VGGish), use this. Uncomment the above\n",
        "\n",
        "for key in files:\n",
        "  for l in dic2[key]:\n",
        "\n",
        "      if len(l)>0:\n",
        "        #maximum = max(maximum,l.shape[0])\n",
        "        pad_width = max_pad_len - l.shape[0]\n",
        "        l = preprocessing(l,flag=True)\n",
        "        mfcc = np.pad(l.T[7:], pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "        audio_data.append(mfcc)\n",
        "      else:                                     # This else statement is for the VGGish features data. They have 28 bad frames. It does not affect the MFCC data\n",
        "        f = np.random.rand(128,858)               \n",
        "        audio_data.append(f)\n",
        "\n",
        "audio_data = np.array(audio_data)         \n",
        "audio_data= audio_data.reshape(audio_data.shape[0],50,max_pad_len)    # For audio_features_joined\n",
        "#audio_data= audio_data.reshape(audio_data.shape[0], 128, 858)            # For audio_pretrained_features_joined"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynhyw_jSu9bT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Join all the labels into a numpy array\n",
        "import numpy as np\n",
        "y = []\n",
        "for key in files:\n",
        "    y+=labels[key]\n",
        "y = np.array(y)\n",
        "\n",
        "#For regression, do not execute any of the lines below\n",
        "\n",
        "y[y>0]=1        #Convert labels to binary\n",
        "y[y<0]=0\n",
        "\n",
        "y=y.astype(int)   \n",
        "\n",
        "#ref = {-3:0,-2:1,-1:2,0:3,1:4,2:5,3:6}         #Uncomment the following three lines for 7 class classification\n",
        "#for i,num in enumerate(y):\n",
        "#  y[i] = ref[num]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gUPZmyHe4i5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to generate train-test split. Arguments are text data,labels,audio features data and split_size (0.8 mean 80:20 train-test split)\n",
        "def split_data(audio,labels,split_size=0.8):\n",
        "  import numpy as np  \n",
        "  train_length =int(len(labels)*split_size)\n",
        "  test_length =int(len(labels)-train_length)\n",
        "  idx = np.random.permutation(labels.shape[0])\n",
        "  audio = audio[idx]\n",
        "  labels = labels[idx]\n",
        "  audio_train = audio[:train_length]\n",
        "  audio_val = audio[train_length:]\n",
        "  labels_train = labels[:train_length]\n",
        "  labels_val = labels[train_length:]\n",
        "  \n",
        "  return audio_train,audio_val,labels_train,labels_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBhoQS6G4m2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "audio_train,audio_val,labels_train,labels_val = split_data(audio_data,y,split_size=0.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAWFqknUEVuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Statistics\n",
        "print('Dimension of Training  Text Data: ',audio_train.shape)\n",
        "print('Dimension of Validation Text Data: ',audio_val.shape)\n",
        "print('Dimension of Training Labels: ',labels_train.shape)\n",
        "print('Dimension of Validation Labels: ',labels_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD33ZJVFkTAu",
        "colab_type": "text"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQo_FSoaXKcu",
        "colab_type": "text"
      },
      "source": [
        "## **Baseline**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15rESw_K4lVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# audio model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "inp = Input((50,1639))           # Dimensions for MFCC data. For VGGish, change it to (128,858)\n",
        "layer = tf.keras.layers.Conv1D(16,3,activation='relu',input_shape=(50,1639))(inp)\n",
        "layer = tf.keras.layers.Conv1D(32,3,activation='relu')(layer)\n",
        "layer = tf.keras.layers.MaxPool1D(2)(layer)\n",
        "#layer = tf.keras.layers.Conv1D(64,3,activation='relu',padding='same')(layer)\n",
        "#layer = tf.keras.layers.Conv1D(128,3,activation='relu')(layer)\n",
        "#layer = tf.keras.layers.MaxPool1D(2)(layer)\n",
        "layer = tf.keras.layers.Dropout(0.5)(layer)\n",
        "layer = tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(64,return_sequences=True))(layer)\n",
        "layer = tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(32))(layer)\n",
        "layer = tf.keras.layers.Dropout(0.5)(layer)\n",
        "layer = tf.keras.layers.Dense(64,activation='relu')(layer)\n",
        "layer = tf.keras.layers.Dropout(0.5)(layer)\n",
        "layer = tf.keras.layers.Dense(32,activation='relu')(layer)\n",
        "layer = tf.keras.layers.Dropout(0.5)(layer)\n",
        "#out = tf.keras.layers.Dense(1,activation='sigmoid')(layer)\n",
        "out = tf.keras.layers.Dense(7,activation='softmax')(layer)\n",
        "#out = tf.keras.layers.Dense(1)(layer)\n",
        "model = Model(inp,out)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1GId9OIx8RP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reduce =tf. keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, mode='auto')\n",
        "#early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=10, mode='auto')\n",
        "\n",
        "#Uncomment one of the next three lines at a time\n",
        "#model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])                #Binary classification\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])            #7 class classifiaction\n",
        "#model.compile(loss=\"mean_absolute_error\",optimizer='adam',metrics=[\"mean_absolute_error\"])             #Regression\n",
        "history=model.fit(audio_train,labels_train, batch_size=256, epochs=50, validation_data=(audio_val,labels_val),callbacks=[reduce])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FyTmwNyXGBM",
        "colab_type": "text"
      },
      "source": [
        "## **Transformer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkhlfX57fZo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class MultiHeadSelfAttention(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads=8):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        if embed_dim % num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
        "            )\n",
        "        self.projection_dim = embed_dim // num_heads\n",
        "        self.query_dense = layers.Dense(embed_dim)\n",
        "        self.key_dense = layers.Dense(embed_dim)\n",
        "        self.value_dense = layers.Dense(embed_dim)\n",
        "        self.combine_heads = layers.Dense(embed_dim)\n",
        "\n",
        "    def attention(self, query, key, value):\n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "        output = tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        query = self.separate_heads(\n",
        "            query, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        key = self.separate_heads(\n",
        "            key, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        value = self.separate_heads(\n",
        "            value, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        attention, weights = self.attention(query, key, value)\n",
        "        attention = tf.transpose(\n",
        "            attention, perm=[0, 2, 1, 3]\n",
        "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
        "        concat_attention = tf.reshape(\n",
        "            attention, (batch_size, -1, self.embed_dim)\n",
        "        )  # (batch_size, seq_len, embed_dim)\n",
        "        output = self.combine_heads(\n",
        "            concat_attention\n",
        "        )  # (batch_size, seq_len, embed_dim)\n",
        "        return output\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsoOSH51fd_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(layers.Layer):\n",
        "    def __init__(self, maxlen, embed_dim,num_heads):\n",
        "        super(Transformer, self).__init__()\n",
        "        \n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.pur1 = tf.keras.layers.Permute((2, 1))\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(64, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(0.2)\n",
        "        self.dropout2 = layers.Dropout(0.5)\n",
        "        self.dropout3 = layers.Dropout(0.5)\n",
        "        self.pool = tf.keras.layers.GlobalAveragePooling1D()\n",
        "        self.dense = tf.keras.layers.Dense(64, activation=\"relu\")\n",
        "        self.out = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "        #self.out = tf.keras.layers.Dense(7, activation=\"softmax\")\n",
        "        #self.out = tf.keras.layers.Dense(1)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        inputs = self.pur1(inputs)\n",
        "        attn_output = self.att(inputs)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        out = self.dropout2(ffn_output)\n",
        "        \n",
        "        out =  self.layernorm2(out1 + ffn_output)\n",
        "        out = self.pool(out)\n",
        "        out = self.dropout3(self.dense(out))\n",
        "        \n",
        "        return self.out(out)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nS6iXtpftgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlen = 1639\n",
        "embed_dim = 50  # Embedding size for each token\n",
        "num_heads = 10  # Number of attention heads\n",
        "\n",
        "inputs = layers.Input(shape=(50,maxlen))\n",
        "transformer_block = Transformer(maxlen, embed_dim,num_heads)\n",
        "outputs = transformer_block(inputs)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA14WrDVf-V_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reduce =tf. keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, mode='auto')\n",
        "#early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=10, mode='auto')\n",
        "\n",
        "#Uncomment one of the next three lines at a time\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])                #Binary classification\n",
        "#model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])            #7 class classifiaction\n",
        "#model.compile(loss=\"mean_absolute_error\",optimizer='adam')             #Regression\n",
        "history=model.fit(audio_train,labels_train, batch_size=12, epochs=50, validation_data=(audio_val,labels_val),callbacks=[reduce])\n",
        "\n",
        "\n",
        "#model_save_filename = \"audio_model.h5\"\n",
        "\n",
        "#earlystopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
        "#mdlcheckpoint_cb = keras.callbacks.ModelCheckpoint(\n",
        "#    model_save_filename, monitor=\"val_accuracy\", save_best_only=True\n",
        "#)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}